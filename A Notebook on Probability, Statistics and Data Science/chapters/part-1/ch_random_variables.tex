\chapter{Distributions of Random Variables} \label{ch:rv}

The definition of random variables has been introduced in the previous chapter. This chapter explores the different types of random variables, their properties, and how they are described mathematically.

\section{Discrete and Continuous Random Variables}

A random variable may be discrete or continuous depending on the sample space of the variable.

\subsection{Discrete Random Variable}

If a random variable $X$ takes only discrete values $x_1, x_2, \ldots$, it is called a \mync{discrete random variable}.
The probability of $X$ taking a particular value $x$ is denoted by $P(X=x)$ or simply $f(x)$ if no ambiguity exists. In this case, $P(X=x)$ and $f(x)$ are the \mync{probability function} (also known as \mync{probability mass function}) of $X$.

Furthermore, define $P(X\leq x)$ or $F(x)$ as the \mync{cumulative distribution function} of $x$. It is easy to prove that $F(x)$ is nondecreasing, and $\lim_{x\rightarrow -\infty}F(x)=0$, $\lim_{x\rightarrow \infty}F(x)=1$. Also, $F(x)$ ``jumps'' at each $P(X=x_k)>0$ and it is continuous from the right.

\subsection{Continuous Random Variable}

A random variable $X$ may also take continuous values in many applications. For example, let $X$ denote the time consumption to finish a task, in which case $X$ is a random variable whose value can be any positive number.

In this case, the probability for $X$ to take a precise value, say finishing a task using precisely 25 minutes 13 seconds 750 milliseconds, is very small (in fact, zero, if the precision approaches infinity). The probability function $P(X=x)$ becomes meaningless. The cumulative distribution function $F(x) = P(X\leq x)$ still makes sense, as it calculates the probability of $X$ within a range rather than at a precise value.

Inspired by this, define \mync{probability density function}[PDF] or $f(x)$ for continuous random variable as follows. 
\begin{eqnarray}
f(x) &=& \dfrac{d}{dx}F(x) \nonumber
\end{eqnarray}
With this definition, $f(x)$ is such a function that
\begin{eqnarray}
  P(a < X < b) &=& \int_{a}^{b}f(x)dx \nonumber
\end{eqnarray}
gives the probability of $X$ in a range, and
\begin{eqnarray}
  F(x) &=& P(X\leq x) \nonumber \\
  &=& \int_{-\infty}^{x} f(\epsilon)d\epsilon \nonumber
\end{eqnarray}
Notice that $f(x)$ itself is not probability. It is $f(x)dx$ accumulating in range $x \in (a, b)$ that forms the probability, hence the name ``probability density''.

In science and engineering problems, continuous random variables are more common than discrete random variables. In engineering, discrete random variables can sometimes be described by impulse PDF.

\subsection{Distribution of Derived Variable}

Let $X$ be a random variable with PDF $f_X(x)$. Let $U$ be another random variable which is a function of $X$, $U=\phi(X)$. The PDF of $U$ can be calculated from $f_X$ and $\phi$. Details are given below.

For simplicity, assume that $U=\phi(X)$ is an injective function (one-to-one function), and $X=\phi^{-1}(U)=\psi(U)$. In that case, the PDF of $U$, $g(u)$, can be obtained as follows.
\begin{eqnarray}
	g(u) &=& \left|\psi\textprime(u)\right|f\left(\psi(u)\right) \nonumber
\end{eqnarray}
For example, let $U=aX$, $X=\frac{U}{a}$.
\begin{eqnarray}
	g(u) &=& \left|\dfrac{1}{a}\right|f\left(\dfrac{u}{a}\right) \nonumber
\end{eqnarray}

Let $X$, $Y$ be two random variables with joint distribution $f(x, y)$. Let $U=X+Y$. The PDF of $U$ can be obtained as follows.
\begin{eqnarray}
	g(u) &=& \int_{-\infty}^{\infty} f(x, u-x)dx \label{eq:conditionalpdf3}
\end{eqnarray}
In the special case where $X$ and $Y$ are independent, $f(x, y) = f_X(x)f_Y(y)$, and \eqref{eq:conditionalpdf3} becomes
\begin{eqnarray}
	g(u) &=& \int_{-\infty}^{\infty} f_X(x)f_Y(u-x)dx \nonumber \\
	&=& f_X * f_Y \nonumber
\end{eqnarray}
where $*$ denotes the convolution operator.

\subsection{Expectation}

Given the probability function of a discrete random variable or the PDF of a continuous random variable, a lot of insights can be extracted. Commonly seen measures of a random variable are introduced in this section and the following sections. Usually, they can be calculated from their associated probability functions or PDFs.

In probability and statistics sense, \mync{expectation} (also known as \mync{mean}) describes the average value of a random variable, if the variable is generated many times. For discrete random variable $X$, the expectation is given below.
\begin{eqnarray}
	\textup{E}(X) &=& \sum_{i=1}^{n}x_iP(x_i) \label{eq:expectationdiscrete}
\end{eqnarray}
where $\textup{E}(\cdot)$ is used to denote the expectation, and $n$ the cardinality of the sample space. In the case of countably infinite sample space, replace $n$ with $\infty$ in \eqref{eq:expectationdiscrete}. For continuous random variable, it is
\begin{eqnarray}
	\textup{E}(X) &=& \int_{-\infty}^{\infty}xf(x)dx \label{eq:expectationcontinuous}
\end{eqnarray}
Expectation is sometimes denoted by $\mu$ in literatures.

Some features of expectation calculation are given below.
\begin{eqnarray}
	\textup{E}(cX) &=& c\textup{E}(X) \nonumber \\
	\textup{E}(X+Y) &=& \textup{E}(X) + \textup{E}(Y) \nonumber
\end{eqnarray}
where $c$ is a constant and $X$, $Y$ are two random variables. These features can be easily derived from \eqref{eq:expectationdiscrete} and \eqref{eq:expectationcontinuous}. Furthermore, if $X$, $Y$ are independent, recall $f(x,y) = f_X(x)f_Y(y)$,
\begin{eqnarray}
	\textup{E}(XY) &=& \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} xyf(x, y)dxdy \nonumber \\
	&=& \int_{-\infty}^{\infty}xf_X(x)dx \times \int_{-\infty}^{\infty}yf_Y(y)dy \nonumber \\
	&=& \textup{E}(X)\textup{E}(Y) \nonumber
\end{eqnarray}

\subsection{Variance and Standard Deviation}

\mync{Variance} and \mync{standard deviation} describe how spread samples are from its expectation. It is defined as follows.
\begin{eqnarray}
	\textup{Var}(X) &=& \textup{E}\left(\left(X-\textup{E}(X)\right)^2\right) \label{eq:vardef} \\
	&=& \textup{E}\left(X^2-2X\textup{E}(X)+\textup{E}(X)^2\right) \nonumber \\
	&=& \textup{E}\left(X^2\right)-\textup{E}(X)^2 \label{eq:varderived} \\
	\textup{Std}(X) &=& \sqrt{\textup{Var}(X)} \nonumber
\end{eqnarray}
Variance and standard deviation are sometimes denoted as $\sigma^2$ and $\sigma$ respectively. Notice that \eqref{eq:varderived} also implies that $\textup{E}\left(X^2\right) \geq \textup{E}(X)^2$, a conclusion used in many lemma derivations.

For continuous random variables, from \eqref{eq:varderived} the variance is
\begin{eqnarray}
	\textup{Var}(X) &=& \int_{-\infty}^{\infty} (x-\textup{E}(X))^2f(x)dx \nonumber
\end{eqnarray}

Some features of variance calculation are given below.
\begin{eqnarray}
	\textup{Var}(cX) &=& c^2\textup{Var}(X) \nonumber
\end{eqnarray}
For independent random variables $X$ and $Y$,
\begin{eqnarray}
	\textup{Var}(X\pm Y) &=& \textup{Var}(X) + \textup{Var}(Y) \nonumber
\end{eqnarray}

Mean and standard deviation can be used to standardize a random variable as follows.
\begin{eqnarray}
	X^* &=& \dfrac{X-\mu}{\sigma} \nonumber
\end{eqnarray}
where $\mu$, $\sigma$ are the mean and standard deviation of random variable $X$ respectively. The standardized random variable, $X^*$, has a mean of $0$ and standard deviation of $1$.

\subsection{Moment} \label{sec:moments}

In mathematics, the \mync{$r$-th moment} of a continuous function $f(x)$ about $c$ is defined as follows.
\begin{eqnarray}
	\mu_n &=& \int_{-\infty}^{\infty}(x-c)^nf(x)dx \nonumber
\end{eqnarray}

By simply saying ``moment'' without specifying $c$, $c=0$ by default. Let $f(x)$ be a PDF. In this sense, the $0$-th order and the $1$st order moment of a probability density function can be calculated as follows.
\begin{eqnarray}
	&& \mu_0 = \int_{-\infty}^{\infty}f(x)dx = 1 \nonumber \\
	&& \mu_1 = \int_{-\infty}^{\infty}xf(x)dx = \textup{E}(X) \nonumber
\end{eqnarray}
where it can be seen that the $0$th and $1$st moments of a PDF are 1 and its mean, respectively.

Further more, let $c=\mu_1$ be the mean of the random variable to calculate the $2$nd-order central moment $\mu_2$ as follows.
\begin{eqnarray}
	\mu_2 = \int_{-\infty}^{\infty}(x-\mu_1)^2f(x)dx = \textup{Var}(X) \nonumber
\end{eqnarray}
which is the variance of the random variable.

Using mean and variance to further define standardized moments as shown below. Define $\bar{\mu}_k$ for $k\geq 3$ as follows.
\begin{eqnarray}
	&& \bar{\mu}_k = \dfrac{\mu_k}{\sigma^k} \nonumber
\end{eqnarray}
where
\begin{eqnarray}
	&& \mu_k = \textup{E}\left((X-\mu_1)^k\right) = \int_{-\infty}^{\infty}(x-\mu_1)^kf(x)dx \nonumber \\
	&& \sigma^k = \left(\mu_2\right)^{\dfrac{k}{2}} = \left(\int_{-\infty}^{\infty}(x-\mu_1)^2f(x)dx\right)^{\dfrac{k}{2}} \nonumber
\end{eqnarray}
with $\mu_1$ and $\mu_2$ the mean ($1$st order moment) and variance ($2$nd order central moment) of the random variable respectively.

The $3$-rd and $4$-th order standardized moments are known as the \mync{skewness} and \mync{kurtosis} of the PDF respectively. In some literatures, skewness is denoted by $\gamma_1 = \bar{\mu}_3$, and kurtosis $\gamma_2 = \bar{\mu}_4$.

The skewness $\gamma_1$ is a measure of asymmetry of the PDF. When $\gamma_1 > 0$ or positive skew, the distribution has a long tail on the right side of the PDF. When $\gamma_1 <0$ or negative skew, the distribution has a long tail on the left side. When $\gamma_1 = 0$, the PDF might be symmetric (but not necessarily so). Examples are given in Fig. \ref{fig:skewness_demo}.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=250pt]{chapters/part-1/figures/skewness_demo.eps}
	\caption{Demonstration of PDF with different skewness.} \label{fig:skewness_demo}
\end{figure}

The kurtosis $\gamma_2$ measures the ``tailedness'' of a probability distribution, i.e., whether the PDF has heavy tail or thin tail. The normal distribution, which has $\gamma_2=3$, is often used as a benchmark. Excess kurtosis is kurtosis subtracting $3$, making the normal distribution having the excess kurtosis of $0$. A positive excess kurtosis would mean a ``heavier'' tail than the normal distribution. Examples are given in Fig. \ref{fig:kurtosis_demo}.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=250pt]{chapters/part-1/figures/kurtosis_demo.eps}
	\caption{Demonstration of PDF with different excess kurtosis.} \label{fig:kurtosis_demo}
\end{figure}

\section{Joint Distribution}

\mync{Joint distribution} describes the distribution of multiple random variable combinations. It is especially useful when these variables are correlated, in which case the joint probability function or PDF can reflect their correlation. 

For example, in a system identification problem the unknown system parameters are correlated with the measurements and their correlations are described by their joint PDF. System identification tries to estimate the unknown system parameters given the measurements.

\subsection{Joint Probability}

\mync{Joint probability} describes the distribution of two or more random variables. For simplicity, in the remainder of this section, two random variables $X$ and $Y$ are considered. The same concepts can be extended to more variables.

In the case of discrete variables $X$ and $Y$, define joint probability function and joint cumulative distribution function as follows.
\begin{eqnarray}
  f(x, y) &=& P\left(X=x, Y=y\right) \nonumber \\
  F(x, y) &=& P\left(X\leq x, Y\leq y\right) \nonumber \\
  &=& \sum_{u\leq x}\sum_{v\leq y}f(u, v) \nonumber
\end{eqnarray}

In the case of continuous random variables, let
\begin{eqnarray}
  F(x, y) &=& P(X \leq x, Y \leq y) \nonumber
\end{eqnarray}
be the cumulative distribution function, and joint PDF of $X$ and $Y$ is defined by
\begin{eqnarray}
	f(x, y) &=& \dfrac{d^2}{dxdy}F(x, y) \nonumber
\end{eqnarray}
Therefore,
\begin{eqnarray}
  \int_{x=a}^{b}\int_{y=c}^{d} f(x, y) dxdy &=& P\left(a < X < b, c < y < d \right) \label{eq:jointpdf} \\
  F(x, y) &=& \int_{u=-\infty}^{x}\int_{v=-\infty}^{y}f(u, v)dudv \nonumber
\end{eqnarray}

The cumulative distribution function and PDF of one of the variables, for example $X$, can be derived from the joint PDF as follows. By definition, 
\begin{eqnarray}
	F_X(x) &=& P(X \leq x) \nonumber \\
	&=& \int_{u=-\infty}^{x}\int_{v=-\infty}^{\infty}f(u, v)dudv \nonumber
\end{eqnarray}
Thus,
\begin{eqnarray}
	f_X(x) &=& \dfrac{d}{dx}F_X(x) \nonumber \\
	&=& \int_{y=-\infty}^{\infty} f(x, y) \label{eq:jointpdfdowngrade}
\end{eqnarray}
which is the integration of \eqref{eq:jointpdf} w.r.t. all other variables from $-\infty$ to $\infty$. 

Equation \eqref{eq:jointpdfdowngrade} can be interpreted as follows. If $(X_i, Y_i)$ samples are generated from $f(x, y)$ in \eqref{eq:jointpdf}, and we only look at the $X_i$ of these samples, they should follow \eqref{eq:jointpdfdowngrade}.

\subsection{Conditional Distribution}

Equation \eqref{eq:jointpdfdowngrade} gives the distribution of $X$ regardless of the value of its corresponding $Y$. Conditional distribution, on the other hand, determines the distribution of $X$ when $Y$ is observed. For example, in \eqref{eq:jointpdf}, calculate $f_{X|Y}(x |Y=y)$, i.e., the PDF of $X$ given $Y=y$. In many literatures, $f_{X|Y}(x |Y=y)$ is denoted by $f_{X|Y}(x|y)$ for simplicity.

The conditional PDF $f_{X|Y}(x|y)$ can be obtained as follows. It is essentially Bayes' rule applied on continuous variables.
\begin{eqnarray}
  f_{X|Y}(x|y) &=& \dfrac{f(x, y)}{f_Y(y)} \label{eq:conditionalpdf1}
\end{eqnarray}
where $f_Y(y)$ is obtained using \eqref{eq:jointpdfdowngrade}. Equation \eqref{eq:conditionalpdf1} is a function of both $x$ and $y$. Substituting the observed value of $y$ into \eqref{eq:conditionalpdf1} reduces it to the PDF of $x$ alone. Its integration with respect to $x$ from $-\infty$ to $\infty$, like any PDF, equals $1$. This can be verified as follows.
\begin{eqnarray}
  \int_{x=-\infty}^{\infty}f_{X|Y}(x|y)dx &=& \int_{x=-\infty}^{\infty}\dfrac{f(x, y)}{f_Y(y)}dx \nonumber \\
  &=& \dfrac{\int_{x=-\infty}^{\infty}f(x, y)dx}{f_Y(y)} \nonumber \\
  &=& \dfrac{f_Y(y)}{f_Y(y)} \nonumber \\
  &=& 1 \nonumber
\end{eqnarray}
Note that given a particular value of $y$, $f_Y(y)$ is a constant value independent of $x$, and hence can be taken out of the integration in the above derivation.

If $X$ and $Y$ are independent variables, $f(x,y) = f_X(x)f_Y(y)$. In this case, \eqref{eq:conditionalpdf1} becomes
\begin{eqnarray}
  f_{X|Y}(x|y) &=& f_X(x) \nonumber
\end{eqnarray}
which implies that the information of $Y=y$ does not affect our understanding of $X$, just as if the information is absent.

Equation \eqref{eq:conditionalpdf1} can be recurrently re-written as
\begin{eqnarray}
f_{X|Y}(x|y) &=& \dfrac{f_{Y|X}(y|x)f_X(x)}{f_Y(y)} \label{eq:conditionalpdf1a}
\end{eqnarray}


\subsection{Covariance and Correlation}

Covariance and correlation are defined for a joint distribution with multiple random variables. For simplicity, consider only two random variables $X$, $Y$ whose joint distribution is given by $f_{XY}(x, y)$. The idea derived from here can be generated to more variables.

The PDF of one variable can be derived from the joint distribution using \eqref{eq:jointpdfdowngrade}. It is straight forward to get the expectation and variance for that variable as follows.
\begin{eqnarray}
	\textup{E}(X) &=& \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xf(x, y)dxdy \nonumber \\
	\textup{Var}(X) &=& \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\left(x-\textup{E}(X)\right)^2f(x, y)dxdy \nonumber
\end{eqnarray}

The \mync{covariance} of two variables is defined and calculated as follows.
\begin{eqnarray}
	\textup{Cov}(X, Y) &=& \textup{E}\left((x - \textup{E}(X))(y - \textup{E}(Y))\right) \label{eq:covariance1} \\
	&=& \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x - \textup{E}(X))(y - \textup{E}(Y))f(x, y)dxdy \label{eq:covariance2}
\end{eqnarray}
where $\textup{Cov}(X, Y)$ is sometimes denoted by $\sigma_{XY}$. Notice that unlike variance that is always positive, covariance can be zero or negative. If $X$, $Y$ are independent variables, $f(x,y) = f_X(x)f_Y(y)$. From \eqref{eq:covariance2}
\begin{eqnarray}
	\textup{Cov}(X, Y) &=& \int_{-\infty}^{\infty}(x - \textup{E}(X))f_X(x)dx \times \int_{-\infty}^{\infty}(y - \textup{E}(Y))f_Y(y)dy \nonumber \\
	&=& 0 \nonumber
\end{eqnarray}
If the covariance of the two variables is zero, the two variables are called \mync{uncorrelated}. Independent variables are always uncorrelated. However, uncorrelated variables are not necessarily independent.

Furthermore,
\begin{eqnarray}
	\textup{Cov}(X, Y)^2 &\leq& \textup{Var}(X)\textup{Var}(Y) \nonumber
\end{eqnarray}
The proof is given below. Notice that lemma \eqref{eq:explemma} is used in the proof.
\begin{mdframed}[frametitle={Lemma}]
	
	For two random variables $X$ and $Y$,
	\begin{eqnarray}
		\textup{E}(XY)^2 &\leq&  \textup{E}(X^2)\textup{E}(Y^2) \label{eq:explemma}
	\end{eqnarray}
	\noindent Proof:
	\begin{eqnarray}
		0 &\leq& \textup{E}\left(\left(X-Y\dfrac{\textup{E}(XY)}{\textup{E}(Y^2)}\right)^2\right) \nonumber \\
		&=& \textup{E}\left(X^2-2XY\dfrac{\textup{E}(XY)}{\textup{E}(Y^2)}+\textup{E}(Y^2)\dfrac{\textup{E}(XY)^2}{\textup{E}(Y^2)^2}\right) \nonumber \\
		&=& \textup{E}(X^2) - 2\textup{E}(XY)\dfrac{\textup{E}(XY)}{\textup{E}(Y^2)} + \textup{E}(Y^2)\dfrac{\textup{E}(XY)^2}{\textup{E}(Y^2)^2} \nonumber \\
		&=& \textup{E}(X^2) - 2\dfrac{\textup{E}(XY)^2}{\textup{E}(Y^2)} + \dfrac{\textup{E}(XY)^2}{\textup{E}(Y^2)} \nonumber \\
		&=& \textup{E}(X^2) - \dfrac{\textup{E}(XY)^2}{\textup{E}(Y^2)} \nonumber
	\end{eqnarray}
	Therefore
	\begin{eqnarray}
		\dfrac{\textup{E}(XY)^2}{\textup{E}(Y^2)} &\leq& \textup{E}(X^2) \nonumber \\
		\textup{E}(XY)^2 &\leq&  \textup{E}(X^2)\textup{E}(Y^2) \nonumber
	\end{eqnarray}
\end{mdframed}
Using \eqref{eq:explemma} on \eqref{eq:covariance1}, \eqref{eq:vardef} gives
\begin{eqnarray}
	\textup{Cov}(X, Y)^2 &=& \textup{E}\left((x - \textup{E}(X))(y - \textup{E}(Y))\right)^2 \nonumber \\
	&\leq& \textup{E}\left((x - \textup{E}(X))\right)^2\textup{E}\left((y - \textup{E}(Y))\right)^2 \nonumber \\
	&=& \textup{Var}(X)\textup{Var}(Y) \nonumber
\end{eqnarray}
or equivalently
\begin{eqnarray}
	\sigma_{XY}^2 &\leq& \sigma_X^2\sigma_Y^2 \nonumber
\end{eqnarray}
Noticing that while $\sigma_X$, $\sigma_Y$ are always nonnegative, $\sigma_{XY}$ is not,
\begin{eqnarray}
	-\sigma_X\sigma_Y \leq \sigma_{XY} \leq \sigma_X\sigma_Y \label{eq:covlemma}
\end{eqnarray}

The \mync{correlation} of two variables is defined and calculated as follows.
\begin{eqnarray}
	\rho &=& \dfrac{\textup{Cov}(X, Y)}{\sqrt{\textup{Var}(X)}\sqrt{\textup{Var}(Y)}} \nonumber \\
	&=& \dfrac{\sigma_{XY}}{\sigma_X\sigma_Y} \nonumber
\end{eqnarray}
From \eqref{eq:covlemma}, apparently $-1\leq \rho \leq 1$. When two variables are uncorrelated or independent, $\rho=0$. If $\rho=1$, the two variables $X$ and $Y$ are said to be \mync{perfect positive correlated}. This happens usually because the two variables are positively linearly depended, for example, $X=2Y$ or $X=Y+1$. If $\rho=-1$, they are said to be \mync{perfect negative correlated}, and the similar idea applies.

\section{Parameter Estimation}

Parameter estimation is an important use case of conditional distribution. Let $\theta$ be the parameter to be estimated, and $x$ the measurement that reflects $\theta$ via measurement model $f(\theta, x)$ which is given in the form of joint PDF. Both $\theta$ and $x$ can be vectors.

\subsection{Posteriori Mean}

Without measurement $x$, the estimate of $\theta$ is given by
\begin{eqnarray}
	\hat{\theta} &=& \int_{-\infty}^{\infty} \theta f_\theta(\theta) d\theta \nonumber
\end{eqnarray}
where $f_\theta(\theta)$ is obtained using \eqref{eq:jointpdfdowngrade}. This is known as the \mync{priori estimation} of $\theta$. 

Given measurement $x$, the \mync{posteriori estimation} of $\theta$ can be obtained as follows. From \eqref{eq:conditionalpdf1a},
\begin{eqnarray}
	f_{\theta|X}(\theta|x) &=& \dfrac{f_{X|\theta}(x|\theta)f_\theta(\theta)}{f_X(x)} \label{eq:conditionalpdf2} \\
	\hat{\theta} &=& \int_{-\infty}^{\infty} \theta f_{\theta|X}(\theta|x) d\theta \label{eq:posterioriestimation}
\end{eqnarray}
where $f_{X|\theta}(x|\theta)$ is known as the \mync{likelihood function} that describes the likelihood of measuring $x$ if the actual parameter(s) is $\theta$. The PDF $f_\theta(\theta)$ is from the priori estimation of $\theta$. Finally, $f_X(x)$ is known as the \mync{evidence}. With fixed $x$, $f_X(x)$ becomes a constant.

Equation \eqref{eq:conditionalpdf2} is essentially
\begin{eqnarray}
	\textup{posteriori} &=& \dfrac{\textup{likelihood}\times\textup{priori}}{\textup{evidence}} \label{eq:posteriorimemo}
\end{eqnarray} 

Notice that all the components in \eqref{eq:conditionalpdf2} and \eqref{eq:posteriorimemo} are derived from the joint distribution $f(\theta, x)$. With known analytical $f(\theta, x)$ and a set of measurement $x$, \eqref{eq:conditionalpdf2} can be used to calculate the estimate. However, in practice $f(\theta, x)$ is often unknown and other estimation methods need to be used.

\subsection{MLE and MAP}

Alternative to \eqref{eq:posterioriestimation}, there are at least two other types of commonly seen estimations, namely \mync{Maximum Likelihood Estimation}[MLE] and \mync{Maximum A Posteriori}[MAP] estimation. MLE and MAP are widely used in system identification. Hence, more details of them can be found in control system relevant notebooks. A brief introduction is given below.

In many occasions, the joint distribution $f(\theta, x)$ is unknown. Instead, the measurement $x$ can be formulated as a function of $\theta$ plus noise. In this case, only $f_{X|\theta}(x|\theta)$ the likelihood function can be derived, whereas the priori estimation and evidence are unknown. As a result, \eqref{eq:conditionalpdf2} and \eqref{eq:posterioriestimation} cannot be used to calculate the estimate.

In this case, consider maximizing the likelihood function to obtain the estimate as follows.
\begin{eqnarray}
	\hat{\theta} &=& \argmax_\theta f_{X|\theta}(x|\theta) \nonumber
\end{eqnarray}
This is known as MLE. MLE is useful when only the output model is known.


There are many ways to solve an MLE problem. In the special cases where $f_{X|\theta}(x|\theta)$ is a special distribution, such as Gaussian, Laplace, etc., the MLE becomes \mync{Weighted Least Squares}[WLS] estimator, \mync{Least Absolute Value}[LAV] estimator, etc., respectively.

A general way of solving MLE is given below. Let cost function
\begin{eqnarray}
	J &=& -\sum_{i=1}^{m}\textup{ln}f_{X|\theta}(x|\theta) \nonumber \\
	\hat{\theta} &=& \arg\min_{x} J \nonumber
\end{eqnarray}
which can be solved using commonly used optimization algorithms such as Newton-Raphson Method.

MAP estimation is also known as the Bayesian estimation. Like the posteriori mean method, MAP also calculates the posteriori distribution \eqref{eq:conditionalpdf2}. But instead of \eqref{eq:posterioriestimation}, the estimate is calculated using
\begin{eqnarray}
	\hat{\theta} &=& \argmax_\theta f_{\theta|X}(\theta|x) \nonumber \\
	&=&  \argmax_\theta \dfrac{f_{X|\theta}(x|\theta)f_\theta(\theta)}{f_X(x)} \label{eq:mapestimate1}
\end{eqnarray}

Notice that \eqref{eq:mapestimate1} is equivalent to
\begin{eqnarray}
	\hat{\theta} &=& \argmax_\theta f_{X|\theta}(x|\theta)f_\theta(\theta) \label{eq:mapestimate2}
\end{eqnarray}
as $f_X(x)$ becomes constant for a given set of $x$. In this sense, comparing with the posteriori mean method, MAP does not require $f_X(x)$ nor the joint distribution $f(\theta, x)$ in the calculation. Comparing with MLE, MAP further requires $f_\theta(\theta)$ the priori estimate.

Equation \eqref{eq:mapestimate2} can be solved as follows.
\begin{eqnarray}
	J &=& -\sum_{i=1}^{m}\textup{ln}f_{X|\theta}(x|\theta) - \textup{ln}f_\theta(\theta) \nonumber \\
	\hat{\theta} &=& \arg\min_{x} J \nonumber
\end{eqnarray}
likewise.

\begin{mdframed}
	\noindent \textbf{MLE versus MAP versus Posteriori Mean}
	
	It is clear that MLE require the least information among the three to carry out the calculation. MAP requires more information. Posteriori mean method requires the most information.
	
	MLE maximizes the likelihood function $f_{X|\theta}(x|\theta)$ only. MAP maximizes the product of likelihood function and the priori estimation, $f_{X|\theta}(x|\theta)f_\theta(\theta)$. From that sense, MLE can be taken as a special case of MAP where $f_\theta(\theta)$ is constant for all $\theta$.
	
\end{mdframed}

\subsection{Parameter Estimation Benchmark}

Estimation error is almost always unavoidable due to the limited size of the samples. However, it is desirable that with an appropriate estimation method, the estimates are unbiased from the true value of the population, i.e., the expectation of the estimates should equal the corresponding values of the population. In that case, the estimation is known as \mync{unbiased estimation}. 

Consider unbiased estimation. The accuracy of an estimation method can be evaluated as follows.

The maximum information contained in the samples can be described by \mync{Fisher Information Matrix} named after Sir Ronald Fisher. The following equation describes the relation between $\textup{Var}(\hat{\theta})$ and Fisher Information Matrix $I(\theta)$ for a scalar unbiased estimator.
\begin{eqnarray}
	\textup{Var}(\hat{\theta}) &\geq& I(\theta)^{-1} \label{eq:crlb}
\end{eqnarray}
which is known as the \mync{Cram\'{e}râ€“Rao lower bound}[CRLB] of the estimator. From \eqref{eq:crlb}, it can be seen that the ``larger'' $I(\theta)$, the lower $\textup{Var}(\hat{\theta})$  can possibly reach.

It is possible (although in many cases difficult) to check and prove whether an estimator hits the bound and provides the smallest possible $\textup{Var}(\hat{\theta})$ by comparing it with its corresponding CRLB. If its variance equals to CRLB, the estimator an \mync{optimal estimator}. A more general CRLB for biased estimator can also be derived. The detailed derivations of Fisher Information Matrix and general CRLB are not given in this notebook.

The magnitude of the estimation error can be evaluated using \mync{Mean Squared Error}[MSE] and \mync{Root Mean Squared Error}[RMSE] as defined below. 

Let $\theta$ be a parameter and $\hat{\theta}_i$ its estimation in the $i$th Monte-Carlo run. Let $n$ be the total number of Monte-Carlo runs. Define MSE and RMSE over the $n$ Monte-Carlo runs as follows.
\begin{eqnarray}
	\textup{MSE} &=& \dfrac{1}{n}\sum_{i=1}^{n}\left(\hat{\theta}_i-\theta\right)^2 \nonumber \\
	\textup{RMSE} &=& \sqrt{\textup{MSE}} \nonumber
\end{eqnarray}
Notice that in practice, MSE and RMSE are meaningful only when $n$ is large enough so that their values converge. Let $n$ approaches infinity and we can observe that
\begin{eqnarray}
	\textup{Var}(\hat{\theta}) = E\left[\left(\hat{\theta}-E\left[\hat{\theta}\right]\right)^2\right] = E\left[\left(\hat{\theta}-\theta\right)^2\right] = \textup{MSE} \nonumber
\end{eqnarray}
for unbiased estimation because $E\left[\hat{\theta}\right]=\theta$, and the MSE approaches the variance of the estimate. From this point of view, we can use the variance of the estimate, $\textup{Var}(\hat{\theta})$, interchangeably with MSE as an evaluation of the performance of the estimation. In most literatures, $\textup{Var}(\hat{\theta})$ is used, implying that the result is theoretical and derived under the assumption of ``infinite runs''.

The smaller $\textup{Var}(\hat{\theta})$, the more precise the estimation. There is a limitation to how small $\textup{Var}(\hat{\theta})$ can be regardless of the design of the estimator. This is intuitive because there is only so much information contained in the samples, given limited sample size and measurement error. So long as the estimation is done using those samples, its precision is limited. 

\section{Important Theorems}

There are a few important theorems frequently used in the study of probability and statistics. They are introduced here.

\subsection{Law of Large Numbers} \label{subsec:largenumbers}

The \mync{Law of Large Numbers}[LLN] is a theorem that basically says if performing the same experiment a large number of times, the average of the outcomes of the experiments should eventually converge to a certain value which is the empirical expectation of the experiment. The larger number of trails, the closer the average to the empirical expectation.

In mathematical expression, let $X$ be a random variable which represents the outcome of an experiment. Let $X_i$ be a sample of the outcome. According to LLN,
\begin{eqnarray}
	\lim_{n\rightarrow\infty} \sum_{i=1}^{n}\dfrac{X_i}{n} &=& \bar{X} \nonumber
\end{eqnarray}

\subsection{Central Limit Theorem}

\mync{Central Limit Theorem}[CLT] states the following observation. For \myabb{independent and identically distributed}{i.i.d.} random variables not necessarily following normal distribution, the empirical mean of the samples taken from these distributions tends towards normal distribution when the number of samples is large.

Let $X$ be a random variable not necessarily following normal distribution, and it has mean and variance of $\mu$ and $\sigma^2<\infty$ respectively. Let $X_i$ be samples of the random variable. The empirical mean of the samples is calculated by
\begin{eqnarray}
	\bar{X}_n &=& \dfrac{1}{n}\sum_{i=1}^{n}X_i \nonumber
\end{eqnarray}
CLT states that $\bar{X}_n$ follows normal distribution when $n$ is large. The mean and variance of the normal distribution are $\mu$ and $\dfrac{\sigma^2}{n}$ respectively, i.e.,
\begin{eqnarray}
	\dfrac{\bar{X}_n-\mu}{\dfrac{\sigma}{\sqrt{n}}} \nonumber
\end{eqnarray}
follows standard normal distribution. 
