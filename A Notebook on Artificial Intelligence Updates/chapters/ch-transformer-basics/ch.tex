\chapter{Transformer Basics} \label{ch:transformerbasics}

This chapter introduces the development of ``transformer'' in its main branch. Transformer is an AI framework initially proposed in 2017 in a paper ``Attention is All You Need'', and it has magically revolutionized how we train AI for natural language processing and interpreting.

\section{It Seems That Attention is All You Need}

It started in 2017 when ``transformer'' is proposed in a paper ``Attention is all you need'', by Vaswani, et al., a bunch of guys mainly from Google.

``Attention'' was not a new concept previous to this paper. It was proposed a few years earlier than the transformer, and had been integrated into several CNN and RNN models as a supporting role to help boost their performances to capture long-range dependencies. Hence, it is quite surprising indeed, that when getting rid of CNN and RNN, and purely relying on attention mechanism, everything actually gets better: it is faster to train the network, and its performance gets improved. 

This concludes that when getting rid of RNN and relying on attention mechanism alone, natural language processing actually gets better. Does this mean the RNN, which we had been relying on so much, is in fact the drawback to natural language processing? The following is the answer of ChatGPT-4 of this question.

\begin{shortbox}
	\textbf{Q: By getting rid of RNN and using attention mechanism alone, natural language processing performance gets better. Is this correct? Does this mean that RNN is a drawback to natural language processing?}
	
	A: Yes, by getting rid of RNNs and using the attention mechanism alone, the Transformer model demonstrated improved performance in natural language processing tasks. However, it's important to note that RNNs are not inherently a drawback to NLP; they were successfully used for many tasks before the advent of Transformers.
	
	RNNs, particularly LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) variants, were designed to handle sequences and capture dependencies in the data. They have been effective in many NLP tasks, such as language modeling, machine translation, and sentiment analysis. However, RNNs do have certain limitations, such as difficulty in capturing long-range dependencies and sequential processing, which can lead to slower training and inference times.
	
	The attention mechanism, as employed in the Transformer model, addresses some of these limitations. It enables parallel processing of input sequences, leading to faster training and inference, and effectively captures long-range dependencies without the vanishing gradient problem that can affect RNNs.
	
	While Transformers have shown superior performance in many NLP tasks, RNNs still have their place in certain scenarios, especially when working with resource-constrained environments or smaller datasets. They can be more lightweight and computationally efficient compared to large-scale Transformer models.
	
	In summary, RNNs are not a drawback to NLP per se, but their limitations have been mitigated by the introduction of the attention mechanism in Transformer models, which has led to improved performance in a wide range of NLP tasks.
	
	(April 14, 2023, ChatGPT-4)
\end{shortbox}

\section{Limitations of CNN and RNN}

\section{Attention Mechanism}

\section{Transformer}

\section{Latest Updates and Trends in Research of Transformer}