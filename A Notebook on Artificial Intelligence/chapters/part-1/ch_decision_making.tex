\chapter{Decision Making}

An AI agent should be capable of making decisions. Based on what it observes, what it believes and what it desires, the agent must determine the most beneficial action or sequence of actions to take. Different from a conventional optimal control problem where the plant and the cost function are assumed known, one of the biggest challenges for the AI agent is that it does not possess sufficient prior knowledge about the plant, and in many cases it needs to learn from successful and failed trails using both offline and online data, and gradually improve its performance.

This chapter studies decision making with AI. Markov decision process is discussed in detail, as it is one of the most widely used decision-making frameworks.

\section{Utility}

To formulate a decision-making problem mathematically, the first step is to define the \mync{utility function} that quantifies the value of an action or a system state. Decision making is essentially the process of maximizing utility. This section introduces the concept of utility and its formulation, and how utility is used in decision making with AI.

\subsection{Utility Theory}

Intuitively, a rational AI agent should always choose the action that maximizes expected utility among all available actions. This is known as the \mync{maximum expected utility} (MEU) principle. Below, MEU is formulated as an optimization problem.

When an AI agent decides to perform an action among all the actions it can take, the system transitions from one state to another. Let the \mync{transition model} be denoted by $P(s\textprime|a,e)$, where $e$ represents the evidence or observation of the system, incorporating the agent’s awareness of its current origin state, $a$ the action taken, and $s\textprime$ the destination state. Notice that $P(s\textprime|a,e)$ is given in the form of probability, which captures uncertainty in the system.

The ``value'' of a state is quantitatively described by the \mync{utility of state} $U(s)$. The utility $U(s)$ usually includes not only the immediate reward gained from state $s$, but also the foreseeable future benefits that may arise from reaching $s$ as an intermediate state. For now, do not bother how $U(s)$ can be calculated. Later in Sections \ref{sec:mdp} and \ref{sec:pomdp}, the systematic calculation of $U(s)$ will be introduced.

The \mync{expected utility of action} $EU(a|e)$, given evidence $e$, over possible landing states $s_i \in S$, can be given by
\begin{eqnarray}
	EU(a|e) &=& \sum_{s_i \in S\textprime} P(s_i|a,e) \left(R(s_i,a,e) + \gamma U(s_i)\right) \label{eq:expected_utility}
\end{eqnarray}
where $S\textprime$ is the set of all probable destination states by taking action $a$ given observation $e$, $R(s_i,a,e)$ the reward gained by taking the action $a$ to reach $s_i$ given the observation $e$, $U(s_i)$ the utility of state $s_i$, and $\gamma \leq 1$ the discount factor. More about discount factor will be introduced in later sections. 

\begin{mdframed}
\noindent \textbf{Reward versus Utility}

Reward and utility are two different concepts in the context of this chapter. Reward represents the instantaneous profit gained by the system by performing an action or a transition, or by leaving or landing on a state. Utility, on the other hand, is a comprehensive evaluation of ``how much an action or a state worth'', by considering not only the instantaneous reward, but also expected maximum reward (often with discount rate applied) to be received in the future. 

As will be shown later, utility can be derived from rewards and transition model.

Both reward and utility are relative values and they can be positive, zero or negative.

\end{mdframed}


Given the evidence $e$, an AI agent may take one of several actions $a_i \in A$. MEU suggests that the optimal action is
\begin{eqnarray}
	a^* &=& \argmax_{a_i \in A} EU(a_i|e) \label{eq:emu}
\end{eqnarray}

An example of MEU is given by Fig.~\ref{fig:emuexp}. Consider a maze in which a robotic AI agent is randomly placed in one of several locations (blocks). Each location corresponds to a state represented by the coordinate $(x,y)$. The agent can move horizontally or vertically one step at a time. Let the utility of a state be given by its shortest distance to the goal, as shown in Fig.~\ref{fig:emuexp}. It can be interpreted as the minimum turn required to reach the goal from its current location, if optimal actions are taken. (Notice that this is not how utility of a state is often calculated. It is used only in this demonstrative example.)

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\textwidth]{./chapters/part-1/figures/emuexp.png}
	\caption{State utility map in the MEU example.}
	\label{fig:emuexp}
\end{figure}

Let $e$ represent the agent’s current location, assumed to be the red dot at $(2,1)$. The agent can take four actions and they are listed below together with their possible resulting states and associated probabilities.
\begin{eqnarray}
	a_1 &=& \mathrm{UP} \nonumber \\
	P((2,1)|a_1,e) &=& 0.2 \nonumber \\
	P((2,2)|a_1,e) &=& 0.6 \nonumber \\
	P((1,1)|a_1,e) &=& 0.1 \nonumber \\
	P((3,1)|a_1,e) &=& 0.1 \nonumber
\end{eqnarray}
\begin{eqnarray}
	a_2 &=& \mathrm{LEFT} \nonumber \\
	P((2,1)|a_2,e) &=& 0.2 \nonumber \\
	P((2,2)|a_2,e) &=& 0.1 \nonumber \\
	P((1,1)|a_2,e) &=& 0.6 \nonumber \\
	P((3,1)|a_2,e) &=& 0.1 \nonumber
\end{eqnarray}
\begin{eqnarray}
	a_3 &=& \mathrm{DOWN} \nonumber \\
	P((2,1)|a_3,e) &=& 0.7 \nonumber \\
	P((2,2)|a_3,e) &=& 0.1 \nonumber \\
	P((1,1)|a_3,e) &=& 0.1 \nonumber \\
	P((3,1)|a_3,e) &=& 0.1 \nonumber
\end{eqnarray}
\begin{eqnarray}
	a_4 &=& \mathrm{RIGHT} \nonumber \\
	P((2,1)|a_4,e) &=& 0.2 \nonumber \\
	P((2,2)|a_4,e) &=& 0.1 \nonumber \\
	P((1,1)|a_4,e) &=& 0.1 \nonumber \\
	P((3,1)|a_4,e) &=& 0.6 \nonumber
\end{eqnarray}

Note that the outcome of an action can be nondeterministic, as is demonstrated by this example. In practice, this may result from signal transmission errors or environmental uncertainty. The agent cannot move downward from its current location $(2,1)$ since it is already at the bottom boundary of the maze. When it chooses the ``DOWN'' action $a_3$, it will likely remain in the same place after running into the wall.

Using \eqref{eq:expected_utility}, the expected utility of $a_1$ can be computed as
\begin{eqnarray}
	EU(a_1|e) &=& 0.2 \times (-5) + 0.6 \times (-6) + 0.1 \times (-6) + 0.1 \times (-4) \nonumber \\
	&=& -5.6 \nonumber
\end{eqnarray}
Similarly, $EU(a_2|e) = -5.6$, $EU(a_3|e) = -5.1$, and $EU(a_4|e) = -4.6$. Therefore, according to \eqref{eq:emu}, the rational decision is to take action $a_4$, i.e., move “RIGHT.”

In this example, all states $s_i$, transition probabilities $P(s_i|a,e)$, and utilities $U(s_i)$ are assumed known from the very beginning of the game. This is rarely the case in practical problems. Sections~\ref{sec:mdp} and~\ref{sec:pomdp} will introduce variety of methods such as value iteration and Q-learning which will become handy when the aforementioned information is unknown from the beginning.

\begin{mdframed}
	\noindent \textbf{Are there alternatives to MEU?}
	
	The expected utility of an action is calculated using \eqref{eq:expected_utility}. An AI agent may use \eqref{eq:expected_utility} and \eqref{eq:emu} to make decisions. However, can we base decision making on other measures of utility? For example, consider the worst-case utility defined by
	\begin{eqnarray}
		U^{\mathrm{worst}}(a|e) &=& \min\{R(s_i,a,e) + \gamma U(s_i)\,|\,P(s_i|a,e) > 0\} \nonumber
	\end{eqnarray}
	Instead of maximizing expected utility, could we maximize worst-case utility?
	
	This question concerns the definition of a rational agent. To formalize this, the following \mync{axioms of utility theory} is defined.
	\begin{itemize}
		\item \textbf{Orderability.} For any two actions $a_1$ and $a_2$, exactly one of the following statements must hold: “$a_1$ is preferred over $a_2$,” “$a_1$ and $a_2$ are indifferent,” or “$a_2$ is preferred over $a_1$.” Thus, any two actions can be ranked.
		\item \textbf{Transitivity.} If $a_1$ is preferred over $a_2$ and $a_2$ is preferred over $a_3$, then $a_1$ must be preferred over $a_3$.
		\item \textbf{Continuity.} For three actions $a_1$, $a_2$, and $a_3$, where $a_1$ is preferred over $a_2$ and $a_2$ over $a_3$, there exists a probability $p$ such that a compound action $a_{1,3,p,1-p}$ (which randomly chooses $a_1$ with probability $p$ and $a_3$ with $1-p$) is indifferent to $a_2$.
		\item \textbf{Substituability.} If $a_1$ and $a_2$ are indifferent and $a_3$ is any action, then for any $p$, the compound actions $a_{1,3,p,1-p}$ and $a_{2,3,p,1-p}$ must be indifferent.
		\item \textbf{Monotonicity.} If $a_1$ is preferred over $a_2$, then for compound actions $a_{1,2,p_1,1-p_1}$ and $a_{1,2,p_2,1-p_2}$, if $p_1 > p_2$, then $a_{1,2,p_1,1-p_1}$ is preferred over $a_{1,2,p_2,1-p_2}$.
		\item \textbf{Decomposability.} Compound actions can be nested or decomposed. For example, in compound action $a_{1,2,p,1-p}$, if $a_2$ is itself a compound action $a_2 = a_{21,22,q,1-q}$, then $a_{1,2,p,1-p}$ is indifferent to $a_{1,21,22,p,(1-p)q,(1-p)(1-q)}$.
	\end{itemize}
	
	Let $U(a|e)$ (or simply $U(a)$) denote the utility of action $a$. The specific form of $U(a)$, whether it represents expected utility or another valid measure, is acceptable as long as the following conditions are satisfied:
	\begin{itemize}
		\item $U(a)$ must exist for every action.
		\item $U(a)$ must reflect preference: if $U(a_1) > U(a_2)$, then $a_1$ is preferred over $a_2$; if $U(a_1) = U(a_2)$, then they are indifferent.
		\item The preferences reflected by $U(a)$ satisfy the axioms of utility theory.
	\end{itemize}
	
	For any utility $U(a)$ satisfying these criteria, the utility of a compound action composed of actions $a_1, \dots, a_n$ with corresponding probabilities $p_1, \dots, p_n$ is given by
	\begin{eqnarray}
		U(a_{1,2,\dots,n,p_1,p_2,\dots,p_n}) &=& \sum_i p_i U(a_i) \nonumber
	\end{eqnarray}
	
	Note that the utility function is not unique. For instance, given a utility function $U(a)$,
	\begin{eqnarray}
		U'(a) &=& \alpha U(a) + \beta, \quad \alpha > 0 \nonumber
	\end{eqnarray}
	is also a valid utility function.
	
\end{mdframed}

\subsection{Utility Function}

As introduced in the beginning of this chapter, the utility function maps an action or a system state to a real number. The expected utility of an action $U(a|e)$ is an example of the utility function. As noted earlier, the expected utility of an action is not the only valid measure of an action. Nonetheless, it is the most widely used utility measure in commonly seen decision making frameworks. For the remainder of this chapter, unless otherwise specified, we consider expected utility as the utility function of an action.

From \eqref{eq:expected_utility}, the expected utility of an action $EU(a|e)$ depends on the transition model $P(s\textprime|a,e)$ and the destination state utility $U(s\textprime)$. State utility typically includes both the immediate reward (from reaching that state) and the anticipated future utility obtainable from using that state as an intermediate step.

The computation of utility functions for actions and states is often referred to as \mync{preference elicitation}. Different models have different ways of defining and calibrating utility functions, some of which will be introduced in later Sections \ref{sec:mdp} and \ref{sec:pomdp}.

Some general principles for assigning rewards and calculating utility function values are given below.
\begin{itemize}
	\item Although rewards are relative and can take any number, it is often helpful to define global upper and lower bounds, where the lower bound represents ``immediate loss'' and the upper bound represents ``goal achieved''. Rewards and utilities should be scaled accordingly.
	\item In many real-world problems, reward is expressed in monetary terms (financial gain or cost). Rewards and penalties of various types are converted to monetary values and normalized by a scaling factor.
	\item Decision makers may differ in risk preference. Risk-averse and risk-seeking agents can have different rewards even under identical circumstances.
	\item Mathematically derived rewards and utilities sometimes contradict human intuition, as humans are not always rational and do not always conform to the axioms of utility theory.
\end{itemize}

An \mync{influence diagram} or \mync{decision network} represents the structure of rewards or utility functions. It is a graphical model illustrating the relationships between utility, its contributing attributes, and the factors influencing those attributes, as well as the entities responsible for making decisions. Understanding this structure helps decision makers identify which factors influence outcomes and should be considered when determine the utility values.

An example is shown in Fig.~\ref{fig:decisionnetworkexp}, where a decision network assists a buyer in choosing a house. Here, the utility function depends on three attributes—location, construction quality, and price—each determined by several underlying factors. The entity ``buyer'' controls these attributes, selecting actions (purchase choices) that maximize utility.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{./chapters/part-1/figures/decision_network_exp.png}
	\caption{A simple decision network in which a buyer must decide which house to purchase.}
	\label{fig:decisionnetworkexp}
\end{figure}

\section{Markov Decision Process} \label{sec:mdp}

Up to this point, we have introduced the basic principles of decision making. In summary, utility must be defined for each action, and it should be designed to satisfy the axioms of utility theory so that the agent behaves rationally. In practice, the utility of system states is specified, and the utility of an action can be derived from state utilities and the transition model. The decision network helps formulate the utility of a state. The action with the highest expected utility is often regarded as optimal, and this principle is known as MEU.

Equations \eqref{eq:expected_utility} and \eqref{eq:emu} are a realization of the above, with Fig.~\ref{fig:emuexp} providing an example. In that example, the utilities of all states and the transition model are assumed known, which is not always true in reality. While the example in Fig.~\ref{fig:emuexp} is simple, real-world problems are often more complex. Consider a similar setting to Fig.~\ref{fig:emuexp}, except that
\begin{itemize}
	\item The state space is unknown in advance. 
	
	The size, shape, and boundary of the maze are initially unknown, and the agent must explore by trial and error.
	
	\item The transition model is unknown in advance. 
	
	There will be a list of available actions provided to the agent. However, the agent does not know which and how each action will lead it into the next state, and the agent must explore by trail and error.
	
	\item Both the reward and the utility of states and actions are unknown in advance.
	
	The agent does not know the reward it will gain for each transition, until that transition is performed during the trial and error. And since both reward and transition model is unknown, the agent cannot calculate the utilities of states and actions in advance. All the information needs to be gained through exploration.
\end{itemize}
These factors substantially increase the difficulty of the problem.

This section and the next Section~\ref{sec:pomdp} study MDP and discuss the calibration of state utilities and the transition model.

\subsection{General Review of MDP} \label{sec:generalreviewmdp}

A \mync{Markov Decision Process} (MDP) refers to the following decision-making problem formulation:
\begin{itemize}
	\item It defines the state, which is a minimal set of information that reflects the current status of the system. 
	\item It defines the transmission model, which describes the relationship between the current state, the action taken, and the resulting next state. The transition model is often given by the stochastic model $P(s\textprime|s,a)$ (essentially the same as $P(s\textprime|a,e)$ in \eqref{eq:expected_utility}, where current-state information is incorporated into the evidence $e$). The transition model is Markovian, i.e., it depends only on the current state and action, not on the full history.
	\item Agent utility is defined, which is the accumulation of rewards that agent gains by traveling among states. For each travel, the agent get some reward which is determined by the destination state, the action taken, the origin state, or all of them jointly.
	\item Actions are taken sequentially until a time horizon is reached or certain amount of utility is accumulated (often when the agent reaches a particular state).
	\item The objective is to determine the policy at each state. The policy maps each state to the best action that maximizes the expected utility.
\end{itemize}

Depending on whether the agent has the full picture of the current state, the MDP formulations can be divided into two types, namely fully observable MDP and \myabb{partially observable MDP}{POMDP}. This section studies fully observable while the next Section ~\ref{sec:pomdp} studies POMDP. 

\begin{mdframed}
	\noindent \textbf{Known MDP versus Fully Observable MDP}
	
	Notice that a fully observable MDP assumes that the agent always knows its current state but does not necessarily know the transition model or reward function. It does not need to know all possible states either. These quantities can be learned or calibrated through trial and error using reinforcement learning.
	
	If a fully observable MDP also has complete knowledge of all transition models at every state and all possible rewards for each state–action pair, it becomes a \mync{known MDP}. Figure~\ref{fig:emuexp} illustrates an example of a known MDP, where the maze layout and the goal position are both clearly marked. In this case, the agent does not need to explore, and it can compute the optimal policy before taking any action.
	
	Now consider a similar scenario in which the robot does not know the size or shape of the maze, nor the positions of walls or goals in advance. This means the agent does not know the set of reachable states, the transition model (i.e., which actions are possible in each state and their outcomes), or the reward function. It still, however, knows its current position. In this case, the system remains a fully observable MDP, but it is no longer a known MDP.
	
	\vspace{0.1in}
	\noindent \textbf{How to Define the State}
	
	A natural question that follows is how to determine what information should be included in the state. After all, if we were to define the transition model or reward function themselves as part of the state (which we should not), a fully observable MDP would degenerate into a known MDP.
	
	The state should include the minimum set of information, so that
	\begin{itemize}
		\item The state is Markovian. The available actions, reward and the transition model are functions of only the current state, but not any historical states or actions taken.
		\item The reward and transition model are stationary to the state.
	\end{itemize}
	If the state definition cannot fulfill the above requirements, consider augment the state with additional information.
	
	The following examples illustrate how to decide what information should be included in the state. Consider a robot navigating a maze. It knows its current position, represented by coordinates, but it does not know the maze layout. 
	
	Consider the following different scenarios for the robot.
	
	\begin{itemize}
		\item Scenario 1: the robot needs to find the goal.
		
		In this case, the state should include only the robot’s current location. With the current position as the state, a stationary transition model can describe how each action leads to a resulting state and what reward the agent receives if it reaches the goal or a trap. The agent may not initially know which positions correspond to walls or traps, but that information is stationary in the sense that when the agent revisits the same location, the same outcomes apply. Through repeated trials, the agent can learn these relationships.
		
		\item Scenario 2: The robot needs to travel as many unique locations as possible within a time horizon.
		
		In this example, it is insufficient for the state to include only the current location. Although the transition model can still describe how actions lead to new positions, this information alone cannot determine the reward, since the reward depends on whether the robot visits new locations or revisits old ones.
		
		Consider defining the state as the current location together with the total number of unique positions it has visited. Indeed, the state provides enough information to calculate the reward. However, this time it does not possess enough information to calculate the transition model. Assume that the agent takes an action and arrives at a new location. Should that number increase by one? It does not know, because the agent is not sure whether it has arrived the location earlier. The transition model is not Markovian.
		
		Therefore, to preserve the Markov property, the state must include the current location along with all the unique locations it has visited. Only then can both the transition model and the reward function be expressed in a Markovian and stationary form.
		
		\item Scenario 3: The robot needs to find the goal, but the layout of the maze changes over time and other factors.
		
		In this scenario, the positions of walls change according to certain external conditions (e.g., time, temperature, or other factors). The wall configuration directly affects the transition model. If the state includes only the current location of the robot, the transition model becomes non-stationary, violating the MDP assumption. 
		
		The state must therefore include the external conditions that influence the maze layout, ensuring that the transition model remains stationary.
	\end{itemize}
	
As demonstrated by Scenario 3, even if the transition model or reward function of a system is non-stationary (for example, changing with time or other external conditions), the problem can always be reformulated as a stationary MDP by augmenting the state with all relevant variables that cause the non-stationary, such as time or system mode. Therefore, in this notebook, it is assumed without loss of generality that the transition model and the reward function of an MDP are stationary. This assumption is standard in most theoretical formulations and practical implementations of MDP.

If the dynamics of the system is too complicated and there are ``unknown'' factors affecting the transition model and the reward, consider still augmenting those unknown factors in the state and formulate it as a POMDP problem.

\end{mdframed}

\subsection{Problem Formulation}

An MDP is often described by
\begin{eqnarray}
	\left(\mathcal{S}, \mathcal{A}, \mathcal{P}, \gamma, \mathcal{R}\right) \nonumber
\end{eqnarray}
where $\mathcal{S}$ is the set of states, $\mathcal{A}$ actions, $\mathcal{P}$ the transition kernel (denoted by $\mathcal{T}$ in some literatures), $\gamma$ the discount factor, and $\mathcal{R}$ the reward.

State and actions have introduced in details in the earlier Section \ref{sec:generalreviewmdp}. The following concepts are introduced or revisited in this section.
\begin{itemize}
	\item Reward
	\item Utility, Utility of State, Utility of Action
	\item Policy, Utility of Policy, Optimal Policy
\end{itemize}
Some of these concepts have been introduced previously. They are re-emphasized in the specific context of MDP.

\vspace{0.1in}
\noindent \textbf{Reward}
\vspace{0.1in}

A reward is the instantaneous gain or loss obtained by taking an action or staying at a state. Rewards are assumed to be stationary. If rewards vary dynamically (e.g., with time or environmental conditions), the factor causing this variation should be included into the MDP state so that the reward function remains stationary.  

Rewards can take several common forms as follows.
\begin{itemize}
	\item Reward by staying or reaching a state, $R(s)$
	
	In some problems, the agent gains a reward or penalty by reaching or staying at a particular state. The previous state or the action taken to reach that state is irrelevant.  
	For example, in the robot-in-a-maze problem, the robot receives a positive reward when it reaches the goal and a negative reward when it reaches or stays inside a trap. The reward depends only on the destination state, not on the path taken.
	
	\item Reward by taking an action, $R(a)$
	
	In some problems, rewards are associated with performing actions. For example, a robot navigating a maze may receive a small negative reward for each move to represent battery consumption.  
	
	In general, a reward can depend on both the origin and destination states, and the action taken. Such cases are denoted by $R(s, a, s\textprime)$, where the first $s$ is the origin state, $a$ the action, and $s\textprime$ the destination state.
\end{itemize}

\vspace{0.1in}
\noindent \textbf{Utility, Utility of State, Utility of Action}
\vspace{0.1in}

Utility, also known as \mync{value function}[VF] in the context of MDP, differs from reward. While reward represents an immediate benefit gained at a specific step, utility is the cumulative or comprehensive measure of value that reflects the long-term desirability of a state or an action. In other words, utility captures both the current and the expected future rewards achievable from a given state or action. Utility is typically derived or calibrated from rewards but embodies the notion of potential benefit rather than immediate gain.

The following example illustrates the distinction. Consider a game of chess. A reward is obtained only when the game ends in checkmate, positive for the winner and negative for the loser. However, checkmate cannot occur on the first move. If snapshots of the board are taken after each round, the probability of winning gradually increases for one player, from about $50\%$ in the beginning to nearly $100\%$ at checkmate. Although the actual reward is given only at the end, every board configuration during the game has a certain utility. The closer a configuration is to a winning position, the more likely the reward will be gained from that configuration, hence the higher its utility.

The calculation of utility from rewards will be introduced shortly, but before that it is worth mentioning the two types of MDPs with finite and infinite maximum allowed number of actions, as they are treated slightly differently when calculating utility.

\begin{itemize}
	\item Finite horizon
	
	There is a fixed time horizon $N$. After $N$ actions, the game ends and no further utility is accumulated. The goal is to maximize total utility within these $N$ steps.
	
	In this case, although the transition model and reward function can remain stationary, the optimal policy may be non-stationary and vary with the remaining time. Suppose the agent returns to the same state after $k<N$ actions. With only $N-k$ steps left, the optimal policy may differ from what it would have been initially when all $N$ steps were available.
	
	The optimal policy can be made stationary by augmenting the remaining time horizon into the state. When that value becomes zero, no more rewards can be received.
	
	\item Infinite horizon
	
	The game has no explicit time limit, and it can run indefinitely.
	
	In this case, the optimal policy is typically stationary. The optimal action at a given state remains fixed and does not depend on when the state is reached. Care must be taken in defining the utility function, however, because if rewards are unbounded or poorly scaled, the agent may loop indefinitely to accumulate infinite utility, rendering the problem ill-defined.
\end{itemize}

In whichever the case, assume the following scenario. An agent starts with zero utility or reward. It receives $R_0$ right away at the current state $s_0$. It performs action $a_{0,1}$ to transform state from $s_0$ to $s_1$. The action awards a reward $R_{0,1}$. After reaching state $R_1$, it receives a reward $R_1$. It then performs $a_{0,2}$ to transform state from $s_1$ to $s_2$ and it gains $R_{1,2}$. After landing at state $s_2$, it receives $R_2$, and so on. Stationary rewards and transition models are assumed.

The utility the agent collects can be formulated as a collective of rewards as follows.
\begin{eqnarray}
	U &=& R_0 + R_{0,1} + \gamma \left(R_1 + R_{1,2} + \gamma \left(R_2 + R_{2,3} + \gamma(\ldots)\right)\right) \label{eq:rewardtoutility}
\end{eqnarray}
where $0 < \gamma \leq 1$ is known as the \mync{discount factor}. The discount factor has at least two purposes. When $\gamma < 1$ is used, it guarantees that the utility is bounded even in an infinite horizon MDP, so that the agent cannot gain infinite utility by circulating around a few state. It also reflects a widely adopted assumption in finance, where a future reward of the same value is often less worthy than an immediate reward.

Notice that one may formulate \eqref{eq:rewardtoutility} as follows
\begin{eqnarray}
	U &=& R_0 + \gamma \left(R_{0,1} + R_1 + \gamma \left(R_{1,2} + R_2 + \gamma(\ldots)\right)\right) \nonumber
\end{eqnarray}
where the action reward $R_{a,b}$ is treated the same way as $R_{b}$ when applying the discount factor. This expression lives in a parallel world of \eqref{eq:rewardtoutility} and everything should work just alright so long as everything is consistent. Nevertheless, \eqref{eq:rewardtoutility} is more commonly adopted, where we assume that the reward of an action is received together with the origin state reward, not the destination state reward.

Following the spirit in \eqref{eq:rewardtoutility}, the utility of a state can be recurrently defined as follows.
\begin{eqnarray}
	U(s) &=& R(s) + \max_{a\in A(s)}\left(\sum_{s_i \in S} P(s_i|s,a)\left(R(s,a,s_i) + \gamma U(s_i)\right)\right) \label{eq:bellman}
\end{eqnarray}
where $s$ is the state of interest, $U(s)$ the utility of state $s$, $R(s)$ the reward received immediately when reaching the state, $a\in A(s)$ all the actions that can be taken at the state, $s_i \in S$ all the states or all the states reachable from $s$ with action $a$, $P(s_i|s,a)$ the probability of reaching $s_i$ from $s$ with action $a$, $R(s,a,s_i)$ the immediate reward obtained by action $a$ transforming state from $s$ to $s_i$, and $U(s_i)$ the utility of state $s_i$. Equation \eqref{eq:bellman} is known as the \mync{Bellman equation}. Notice that in Bellman equation, it is assumed that the agent is rational and always takes the action that maximizes the utility.

Bellman equation \eqref{eq:bellman} can be used recurrently to calculate the utility of states. More details will be given in Section~\ref{sec:mdpvalue}.

The utility of an action $a$ at state $s$ is given by
\begin{eqnarray}
	U(a|s) &=& \sum_{s_i \in S} P(s_i|s,a)\left(R(s,a,s_i) + \gamma U(s_i)\right) \label{eq:bellmanaction}
\end{eqnarray}
which describes the expected return from taking an action at a state (under a policy). Equation \eqref{eq:bellmanaction} also known as the \mync{Q-function}. Q-function \eqref{eq:bellmanaction} is related to utility of state \eqref{eq:bellman} as follows.
\begin{eqnarray}
	U(s) &=& R(s) + \max_{a\in A(s)} U(a|s) \nonumber
\end{eqnarray}
 
Equation \eqref{eq:bellman} and \eqref{eq:bellmanaction} reveals the correlation between the utility of a state and the utility of an action. The utility of state $s$ is the reward of arriving the state plus the maximum utility of action among all the actions the agent can take from the state.

\vspace{0.1in}
\noindent \textbf{Policy, Utility of Policy, Optimal Policy}
\vspace{0.1in}

A policy refers to a ``rule book'' that says what action to take at each and every state. A policy is often denoted by
\begin{eqnarray}
	\pi(s): && s \rightarrow a \label{eq:mdppolicy}
\end{eqnarray}
as a map from the state $s$ to $a$. The utility of a policy, giving an initial state, can be calculated using \eqref{eq:rewardtoutility}. With stationary reward and transmission model and with a properly chosen discount factor, the utility of a policy is always bounded. 

Among all the probable policies, the policy that gives the highest utility is known as the optimal policy, often denoted by $\pi^*(s)$. It is fairly easy to prove that with stationary reward and transmission model, the optimal policy is also stationary, meaning that when the system is at the same state, the optimal policy should always suggest the same action. In the remainder, only stationary policies are considered.

When the optimal policy is applied, the agent choose the action with the maximum utility from \eqref{eq:bellmanaction}, i.e.,
\begin{eqnarray}
	\pi^*(s) = \argmax_{a\in A(s)} \sum_{s_i \in S} P(s_i|s,a)\left(R(s,a,s_i) + \gamma U(s_i)\right) \label{eq:mdpoptpolicy}
\end{eqnarray}
When there are multiple actions that give the same largest utility, the agent randomly choose one action from them.

\subsection{Value Iteration} \label{sec:mdpvalue}

To get the optimal policy $\pi^*(s)$ using \eqref{eq:mdpoptpolicy}, the key is to get the utility of state $U(s)$. The Bellman equation \eqref{eq:bellman} provides a way to recursively calculate the utility of state as follows.
\begin{eqnarray}
	U_{i+1}(s) &\leftarrow& R(s) + \max_{a\in A(s)}\left(\sum_{s_i \in S} P(s_i|s,a)\left(R(s,a,s_i) + \gamma U_i(s_i)\right)\right) \label{eq:bellmanupdate}
\end{eqnarray}
which is known as the \mync{value iteration} of MDP.

The following example is given to demonstrate the use of Bellman equation to update the utility of states.

\begin{shortbox}
\Boxhead{A Robot in a Maze with Dynamic Rewards and Traps}

Place a robot in a $4\times 4$ maze. 

The robot can move in all directions for one block, or stay at its current space in each round of action. The robot cannot move outside the maze. This defines $9$ actions for the robot if it is in the center area of the maze, $6$ actions if it is beside an edge, and $4$ sections if it is at the corner. When the robot makes an action, there is a probability of $P$ that the command is interpreted correctly and the robot will behave accordingly. There is a probability of $1-P$ that the robot fails to interpret the command, in which case it will randomly select an action from what it can do.

Some locations of the maze are assigned with rewards or penalties. There are two sets of different rewards configurations and they switch every $5$ actions. The two setups are given in Fig.~\ref{fig:valueiterationexp}. The reward map is known in advance.

The robot is able to sense its location and also the time, meaning that it knows not only the current reward setup, but also the future reward map swaps.

The target is to design an MDP and calculate the utility of state using Bellman update. 

\end{shortbox}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{./chapters/part-1/figures/valueiterationexp.png}
	\caption{Reward in the example, where the reward change periodically over time.}
	\label{fig:valueiterationexp}
\end{figure}

Define the state. Notice that the rewards are determined jointly by the location and time cycle. To make the reward stationary, the time cycle information must be included in the MDP state definition. The two reward setups swap every $5$ actions. Therefore, the reward configuration cycle is $10$ actions. There are a total of $4\times 4\times 10=160$ states ($4\times 4=16$ the distinct location of the robot, and $10$ the time index in the reward cycle).

For the convenience, let $x$ a $3\times 1$ vector that can be used to denote the states. The first element $x(1)\in \{1,2,3,4\}$ is the horizontal axis, the second element $x(2)\in \{1,2,3,4\}$ the vertical axis, and the third element $x(3) = \{1,\ldots,10\}\}$ the time index.

Define the transition model. In this example, different states have different available actions. When the robot is at the edge or at the corner, it will have less moving options. For simplicity, only two actions from two states as follows are given as examples below. The rest actions can be formulated similarly.

Consider state $x=[2,2,1]$. This is one of the center area of the maze, and the robot can move $9$ directions from this state (one of them being stay where it is). A total of $9$ actions can be defined for this state. Take ``up'' action as an example. The destination state and the probability for this action is given below.
\begin{eqnarray}
	P\left([1,1,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} \nonumber \\
	P\left([1,2,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} \nonumber \\
	P\left([1,3,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} \nonumber \\
	P\left([2,1,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} \nonumber \\
	P\left([2,2,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} \nonumber \\
	P\left([2,3,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} + P \nonumber \\
	P\left([3,1,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} \nonumber \\
	P\left([3,2,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} \nonumber \\
	P\left([3,3,2]|[2,2,1], \mathrm{up}\right) &=& \dfrac{1-P}{9} \nonumber
\end{eqnarray} 
There are a total of $9$ possible destination state for this action at this state. Notice that the time index increases by $1$ then the action is performed. Consider another state $x=[4,4,10]$. This is a corner state, and the robot can move only $4$ directions from this state. A total of $4$ actions are defined. Take ``left'' as an example. The destination state and the probability for this action is given below.
\begin{eqnarray}
	P\left([3,3,1]|[4,4,10], \mathrm{left}\right) &=& \dfrac{1-P}{4} \nonumber \\
	P\left([3,4,1]|[4,4,10], \mathrm{left}\right) &=& \dfrac{1-P}{4} + P \nonumber \\
	P\left([4,3,1]|[4,4,10], \mathrm{left}\right) &=& \dfrac{1-P}{4} \nonumber \\
	P\left([4,4,1]|[4,4,10], \mathrm{left}\right) &=& \dfrac{1-P}{4} \nonumber
\end{eqnarray} 
The time index loops to $1$ after taking the action.

The reward is formulated as follows. There is no reward for actions $R(s, a, s\textprime)$, and only rewards for certain states $R(s)$ as follows.
\begin{eqnarray}
	R([1,4,i]) &=& 1, i=1,...,5 \nonumber \\
	R([2,2,i]) &=& -10, i=1,...,5 \nonumber \\
	R([3,2,i]) &=& -10, i=1,...,5 \nonumber \\
	R([4,1,i]) &=& -5, i=1,...,5 \nonumber \\
	R([1,4,i]) &=& -5, i=6,...,10 \nonumber \\
	R([2,2,i]) &=& -10, i=6,...,10 \nonumber \\
	R([2,3,i]) &=& -10, i=6,...,10 \nonumber \\
	R([3,2,i]) &=& -10, i=6,...,10 \nonumber \\
	R([4,1,i]) &=& -10, i=6,...,1 \nonumber
\end{eqnarray}

Set discount rate to be $\gamma=0.8$ and success rate of action to be $P=0.9$.

Looping over the entire $160$ states using \eqref{eq:bellmanupdate} is known as one iteration. After $42$ iterations, the utility of the state converges. Fig~\ref{fig:valueiterationexp2} gives some of the results.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{./chapters/part-1/figures/valueiterationexp2.png}
	\caption{Converged utility of state at timestamps $1$,$5$,$6$ and $10$.}
	\label{fig:valueiterationexp2}
\end{figure}

Some highlights are as follows.
\begin{itemize}
	\item The agent tries to stay far away from the ``traps''. There is a fail rate for a move. If the agent stay adjacent to a trap, it may fall into the trap by accident. For safety, the agent prefers to stay or move around at safer areas far away from the traps.
	\item The utility of state is a function of timestamps. Notice that although the reward configurations at timestamp $1$ and $5$ are identical, the utility of states are different. At timestamp $5$, the agent is aware that the reward configuration is going to change, and it is preparing for it. There are several motivations. The original reward sweet point will be a trap, and the agent wants to stay away from it. There is a discount factor, which motivates the agent to move to the new reward point as quickly as possible.
\end{itemize}

When the reward configuration and transition model are known and stationary, we can use value iteration to calculate the utility of state. With utility of state known, it is straight forward to decide the optimal policy using \eqref{eq:mdpoptpolicy}.

It can be proved that if infinite iteration is used, Bellman update will converge to the unique global optimum. Proof is not given here.

\subsection{Policy Iteration}

\mync{Policy iteration} is an alternative way of finding the optimal policy given the reward configuration and transition model. Unlike the value iteration where the utility of the state is first calculated, in policy iteration, it is started with assuming a policy $\pi^0(s)$ which is not necessarily optimal.

With that $\pi^0(s)$, the utility of state can be calculated using \eqref{eq:rewardtoutility}. With the utility of state calculated, use \eqref{eq:mdpoptpolicy} to revise the policy to get $\pi^1(s)$. Repeat the above steps iteratively until the utility of state and the policy converge.

Policy iteration and value iteration should lead to the same result. It is reported that sometimes policy iteration is easier to implement.

\subsection{MDP Online Reinforcement Learning}

Both value iteration and policy iteration assume known reward configuration and transition model, which is often not true. In practice, the agent needs to explore and find the transition model associated with each action at each state as well as the rewards received from each action or state. Inspired by policy iteration, the following \mync{Q-Learning} algorithm can be used.

\begin{enumerate}
	\item In the beginning when there is no information of reward and transition model, initialize utility of state \eqref{eq:bellman} with $0$ for all states. Initialize the Q-function with $0$ for all actions at all states.
	\item Let the agent freely transition from the current state until the end of the session, either when it reaches the goal, runs into a trap, or after certain time. Notice that the action with the largest utility is not necessarily selected. It uses a exploit-explore model instead, which will be introduced later.
	\item For each state visited in the trail, update the Q-function of the actions taken together with the utility of the state as follows, whenever an action is taken.

	\begin{eqnarray}
		U_{k+1}(a|s) &\leftarrow& U_k(a|s) + \alpha_k\left(R(s,a,s_i) + \gamma U_k(s_i) - U_k(a|s)\right) \label{eq:qlearningupdate} \\
		U_{k+1}(s) &\leftarrow& R(s) + \max_{a\in A(s)} U_{k+1}(a|s) \nonumber
	\end{eqnarray}

\end{enumerate}
where steps 2 and 3 are repeated so that the agent keeps learning from different trails, until the agent has gained enough information about each and every action and each and every state, and no further learning is required.

In \eqref{eq:qlearningupdate}, $\alpha_k$ should fulfill the following criteria, so that the Q-function and utility of state converge to the optimum.
\begin{eqnarray}
	&& \sum_{k=0}^{\infty} \alpha_k^2 < \infty \nonumber \\
	&& \sum_{k=0}^{\infty} \alpha_k \rightarrow \infty \nonumber
\end{eqnarray}
which implies that until everything converges, the agent must keep learning. An example of $\alpha_k$ can be $\alpha_k = 1/k \alpha_0$ with a finite $\alpha_0$.

As mentioned earlier, for the agent to keep learning, it should not always stay in its comfort zone by running only the estimated optimal policy. It is encouraged to always explore new actions that is not the best action under existing estimated optimal policy. It is important to maintain a proper exploit-explore balance.

The \mync{$\epsilon$-greedy exploration} model is recommended. In the $k$th trail, for each action the agent is taken, it has a $1-\epsilon_k$ probability of choosing the optimal action under the current estimated optimal policy, and a $\epsilon_k$ probability of choosing a random action, regardless of its Q-function value. The value of $\epsilon_k$ decreases as more and more trails carry out, so that the model will perform more conservative as the learned information accumulates. A common choice is $\epsilon = 1/k$.

Q-Learning is similar with value iteration or policy iteration, except that it is model-free. It is assumed that the reward configuration and transition model are unknown. It uses trails and reinforcement learning to update the utility of actions and utility of states by exploring the different states and transitions of the system.

\section{Partially Observable MDP} \label{sec:pomdp}

\mync{Partially Observable Markov Decision Process}[POMDP] refers to the case where the agent is not always clear which state it is in. Imagine in the example given by Fig~\ref{fig:emuexp}, the agent has no sense of its location in coordinate, but can detect how many walls it is adjacent to. The problem then becomes a typical POMDP. 

POMDP differs from fully observable MDP in several aspects. Though the spirit remains the same where the decision is made based on a Markov process, many details in the execution needs to be adjusted. Details are introduced in this section.

\subsection{Belief of State}

The agent, in this case, has a \mync{belief of state} often denoted by
\begin{eqnarray}
	b(s) &=& \left[p_1, \ldots, p_n\right] \nonumber
\end{eqnarray}
where $n$ is the total number of state and $p_i$ the belief of the agent currently in state $i$. One can think of $b(s)$ as a way to denote the state. The agent makes decisions based on $b(s)$.

The evidence $e$ is the measurement available to the agent. The probability $P(e|s)$ associates the state belief with the evidence. The agent can adjust its belief as follows. Let $b(s)$ be the agent's initial belief. The agent takes action $a$, and after that it observes $e$. The new belief becomes
\begin{eqnarray}
	b\textprime(s\textprime) = \alpha P(e|s\textprime) \sum_{s} P(s\textprime|s,a)b(s) \nonumber
\end{eqnarray}
where $\alpha$ is used for probability normalization so that $\sum b_i(s) = 1$ is ensured. The decision making is Markovian just like the fully observable MDP, where the optimal action depends on only the current belief of state $b(s)$. It does not depend on past states, and it does not depend on actual state.

The biggest challenge is that the $b(s)$ is a group of probability, and hence continuous and has infinite values. For example, the policy $\pi(s)$ (now $\pi(b)$) will be difficult to define, as if it were done in the conventional manner, there will be infinite possibilities. For instance, we cannot assign an action for $b(s) = [0.5,0.5]$ and another action for $b(s) = [0.501, 0.499]$. Also, the transition model which is used to be $P(s\textprime|s,a)$, is now $P(b\textprime|b,a)$ with $b$ a continuous vector.

In the remainder of the section, it is introduced how POMDP defines the transition model and optimal policy in the context of belief of state.

\subsection{Transition Model with Belief of State}

The transition model of actual state $P(s\textprime|s,a)$ and the evidence and actual state correlation $P(e|s)$ are used to derive the transition model of state of belief $P(b\textprime|b,a)$ as follows.
\begin{eqnarray}
	P(b\textprime|b,a) &=& \sum_{e}P(b\textprime|e,a,b)P(e|a,b) \nonumber \\
	&=& \sum_{e}P(b\textprime|e,a,b)\sum_{s\prime}P(e|s\textprime) \sum_{s}P(s\textprime|s,a)b(s) \nonumber
\end{eqnarray}
where
\begin{eqnarray}
	P(e|a,b) &=& \sum_{s\prime}P(e|s\textprime)P(s\textprime|a,b) \nonumber \\
	&=& \sum_{s\prime}P(e|s\textprime) \sum_{s}P(s\textprime|s,a)b(s) \nonumber
\end{eqnarray}

\subsection{Value Iteration and Optimal Policy with Belief of State}

\subsection{POMDP Online Reinforcement Learning}

\section{Continuous-State Markov Decision Process}

Up to this point, both the fully observable MDP and POMDP assume that the number of states is finite. In a more general and practical control problem, however, the number of state can be infinite, mostly because the state variables in the state space vector can take continuous values.

\mync{Continuous-state MDP} describes an MDP formulation where the state variables can take continuous values and hence there are infinite number of states. More details are discussed in the remainder of this section.

\subsection{Problem Formulation}

Consider a control problem where the state space is given by $s\in S=\mathbb{R}^n$. While the order of the system, i.e., the number of the state variable, is a finite number $n$, $s$ can take infinite and continuous values.

The purpose is to decide $a \in A(s)$. There is no parametric model that describes how $a$ affects $s$, and there is no reference signal for the state $s$ either. However, there is a reward $R(s)$ corresponds with each $s$. The reward is perceivable by the system.

From the above description, it is obvious that the problem can hardly be formulated into a parametric model based control problem or an adaptive control problem. It is a reinforcement learning problem by nature. Thus, consider using MDP to formulate and solve the problem. Conventional MDP assumes finite number of state. In this problem the state can take infinite values. Continuous-state MDP formulation is proposed to solve the problem as follows.

\subsection{Discretization Approach}

Intuitively, we can discretize the continuous values the state vector can take, and map them into discrete values. For small scale system where the length of state vector and the ranges each state variable can take is limited, discretization approach may work. However, it has the following problems preventing it from being implemented in large scale systems.
\begin{itemize}
	\item Information loss due to the staircase fitting.
	\item Curse of dimensionality. The total number of states grows exponentially larger as the system scale grows. Consider a system with $n$ state variables, each discretized into $k$ values. The total number of states is $k^n$.
\end{itemize}
The probable solution to the above two problems, under the scope of discretization approach, conflict with each other. To reduce the information loss due to the staircase fitting, the discretization resolution for each state variable needs to be increased, which would cause more severe curse of dimensionality.

In practice, discretization approach only works for small-scale system. For system with order $n\leq 3$, it is often safe to use discretization approach. For system with order $3 < n\leq 6$, there is a chance that it may work. In these cases, the user needs to choose the discretization strategy carefully by using low resolution for state variables that matter less. For system with order $n>6$, discretization approach is generally not recommended.

\subsection{Utility Function Approximation approach} \label{sec:vfapproximation}

Notice that in the remainder of this Section \ref{sec:vfapproximation} finite number of actions is assumed while state vector can take infinite continuous values. This is a fair assumption in many control problems, as the action space is often limited compared with the state vector. In the case where the actions take continuous values, it is often fine to discretize only the action space.

Consider approximating the utility function of a state directly without discretization, i.e.,
\begin{eqnarray}
	U(s) &\approx& \theta^T\phi(s) \label{eq:vfapproximation}
\end{eqnarray}
where $\theta$ is the linear regression coefficient, and $\phi(s)$ the feature of state $s$. With \eqref{eq:vfapproximation}, we can then use \eqref{eq:bellmanaction} and \eqref{eq:mdpoptpolicy} to choose the optimal action and decide the optimal policy.

As the first step to implement utility function approximation, consider building a model of the plant as shown in Fig. \ref{fig:csmdp_model}. The model is also known as the simulator. 

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.6\textwidth]{./chapters/part-1/figures/csmdp_model.png}
	\caption{Model (simulator) of the plaint in the continuous state MDP.}
	\label{fig:csmdp_model}
\end{figure}

Notice that the model is treated like a black box and the user does not need to possess parametric information of the plant. In practice, the model can be constructed purely from data derived from past trajectories when an human operator was running the system. Consider using supervised learning to estimate $s\textprime = f(s,a)$, where $f(\cdot)$ is the plant black box. More about supervised learning is given in Chapters \ref{ch:regression}, \ref{ch:perceptron} and \ref{ch:multilayer-perceptron}.

\begin{mdframed}
\noindent \textbf{Formulations of the Model}

The formulation of the model is not unique especially when it comes from the data. For the same dataset, the user may choose between linear or nonlinear models, and deterministic or stochastic models. There is no globally correct way of formulation for all different applications. In practice, stochastic models like \eqref{eq:stochasticsimulator} are often preferred over deterministic models for enhanced system robustness.

\begin{eqnarray}
s\textprime &=& f(s, a) + \epsilon \label{eq:stochasticsimulator}
\end{eqnarray}

With the built model, we can then train the MDP using reinforcement learning. This is known as the \mync{model-based reinforcement learning}. This is popular in many practical problems when the true plant is too expensive to run. The user can use the model to generate massive amount of synthetic data which gives him flexibility in choosing the feature of the state.

\end{mdframed}

With the defined mode, we need to select finite number of random states $s_1,\ldots,s_m$ from the infinite continuous-value state space. Apply value iteration as follows. For each sampled state $s_i$, let $a\in A(s_i)$ be an action applicable to the state. Iteratively calculate the utility of the action and the state as follows.
\begin{eqnarray}
	U(a|s_i) &\leftarrow& \dfrac{1}{j}\sum_{i=1}^{j} \left[R(s_i, a, s_j) + \gamma U(s_j)\right] \label{eq:csmdpvalueiterationaction} \\
	U(s_i) &\leftarrow& R(s_i) + \max_{a} U(a|s_i) \label{eq:csmdpvalueiteration}
\end{eqnarray}
where $j$ is the number of trial that action $a$ is taken, and $s_k$ the destination state. This can be emulated with the aforementioned model. Massive data is required for the training, as there is often a large number of state samples $m$ and a large number of trials of actions $j$. This is why the model can become handy. The utility $U(s_k)$ is calculated with $U(s_k) = \theta^T\phi(s_k)$ with $\theta$ from the last iteration.

With $U(s_i)$ obtained through value iteration \eqref{eq:csmdpvalueiteration} for the sampled discrete states, consider mapping it with continuous function \eqref{eq:vfapproximation} via linear regression, i.e.,
\begin{eqnarray}
	\theta &=& \argmin_{\theta} \dfrac{1}{2} \sum_{i=1}^{m} \left(\theta^T\phi(s) - U(s_i)\right)^2 \label{eq:csmdpargtheta}
\end{eqnarray}
We need to choose the feature of the state $\phi(s)$. This is often experience based, where $\phi(s)$ can be a subset or an extension of the original $s$ together with nonlinear components derived from components of $s$ that may jointly contribute to the utility. With the massive synthetic data generated from the model, the user can choose a large set of features without necessarily worrying about over-fitting. This is another benefit of using a model.

The number of samples $m$ required by the fitting \eqref{eq:csmdpargtheta}, which is also the number of sampled states, depends on the formulation of $\phi(s)$. For example, for a feature that contains $50$ elements in $\phi(s)$, consider $m$ of at least between $500$ and $1000$.

For online application, the optimal policy is given by
\begin{eqnarray}
	\pi^*(s) &=& \argmax_{a} \left[R(s, a, s_j) + \gamma U(s_j)\right] \label{eq:cspolicy}
\end{eqnarray}
where $s_j = f(s,a)$ is obtained from the model \eqref{eq:stochasticsimulator} with $\epsilon=0$. Notice that although in the training discrete action has been used in the value iteration, \eqref{eq:cspolicy} can be formulated with continuous action space. Details are not discussed here. 

Replace \eqref{eq:csmdpvalueiterationaction} and \eqref{eq:csmdpvalueiteration} with the online greedy-search based counterparts for online learning. Details are not introduced here.

\begin{mdframed}
\noindent \textbf{LQR as a Special Case of Continuous-State MDP}

In a special case of continuous-state MDP where the state transition function and the model can be described by the following linear equation
\begin{eqnarray}
	s\textprime &=& Fs + Ga + w \nonumber
\end{eqnarray}
with $s\in \mathbb{R}^n$, $a\in \mathbb{R}^m$ and the reward
\begin{eqnarray}
	R(s,a) &=& -\left(s^TQ^ss + a^TQ^aa\right) \nonumber 
\end{eqnarray}
where both $Q^s$ and $Q^a$ positive definite matrices, the problem becomes a \mync{Linear Quadratic Regulator}[LQR] problem. Details of LQR can be found elsewhere in optimal control related notebooks and is are not discussed here. 

\end{mdframed}

\section{Multi-attribute Utility Function}

To this point, we have been assuming that all the attributes that affect utility can converted and put on the same scale. When maximizing the utility, only that single figure is considered. However, this is not always true in the reality. For example, consider personal protective equipment design. In practice, there is always a chance that the protection fails. It will take infinite money to make the equipment infinitely safe. To maximize the utility, we need to put financial cost and human life on the same scale, which is difficult and can be immoral.

A problem whose outcomes are characterized by two or more attributes that cannot be easily converted into a single figure are handled by \mync{multi-attribute utility theory}. There are several ways to handle the situation, and they are briefly introduced as follows.

In the heat map approach, the actions are converted into coordinates in a hyper space where each dimension representing an attribute. A heat map is constricted, representing the safe and dangerous zones in the hyper space. Actions inside the safe zone are selected.

In the statistic dominance approach, we investigate the chance that an action may perform better than any other actions in every aspects. The action that statistically more likely to dominant (perform better than other actions in all attributes) is selected.

Sometimes the attributes follow joint distribution. We can set a lower bound for one of the attributes, hence reducing the problem to a single-attribute problem. That attribute is used to formulate the utility function. Consider the earlier personal protective equipment example. The more expensive the gear, the more likely it is safe. The safety and the cost form a positively correlated joint distribution, where each sample in the distribution is an action. We can set a lower bound for safety requirements, for example, safety probability of $99.999\%$, and reduce the problem to a single attribute problem where we find the action with the minimum financial cost that guarantees the safety probability.

\section{Game Theory}

Multi-agent control system. 