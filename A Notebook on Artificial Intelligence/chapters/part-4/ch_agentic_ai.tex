\chapter{Agentic AI} 

Agentic AI describes a comprehensive AI system that can make decisions and perform actions for either general or specific tasks, with minimal human intervention. It is a fundamental building block of autonomous systems.

The concept of agentic AI is not new. However, in recent years it has drawn increasing attention due to the rapid advancements in LLM. LLMs provide a general-purpose language interface that makes agentic AI significantly more scalable and cost-effective. As a result, agentic AI is included as a key topic in this part of the notebook.

\section{Introduction to Agentic AI}

The term ``agent'' refers to an entity that functions like a human. By this definition, agentic AI refers to AI-based systems that can solve complex problems with limited human oversight. It is possible to build an agentic AI system without LLM and use only conventional AI frameworks such as MDP and ANN. While being good at specific tasks, the use of such agentic AI systems often lacks of general-purpose interfaces. With the recent rise of LLMs, the term ``agentic AI'' has been re-contextualized and is gaining increasing popularity. LLMs provide a general-purpose language interface that makes agentic AI significantly more scalable and cost-effective. 

An LLM-incorporated agentic AI system with LLM agents is often more capable than a conventional agentic AI system at least in the following aspects.
\begin{itemize}
	\item An LLM-incorporated agentic AI system can understand human instructions in natural languages.
	\item An LLM agent can take in unstructured data input and generate structured data output in a scalable manner.
	\item An LLM agent can make decisions based on LLM's domain knowledge.
\end{itemize}

Agentic AI in the context of LLMs refers to an autonomous system in which LLMs not only perform tasks, but also participate in decision-making and workflow orchestration. In such systems, LLMs serve both as computational units that solve sub-tasks, and as the high-level orchestrator that determines how the problem should be approached. An LLM in an agentic AI system may take on one or more of the following roles.
\begin{itemize}
	\item Decompose the original problem into smaller sub-problems.
	\item Assign these sub-problems to LLM instances, either sequentially or in parallel.
	\item Evaluate the outputs of LLMs and either accept the results or reject them with feedback and revision instructions.
	\item Integrate the outputs from sub-problems into a final, coherent solution to the original problem.
	\item Along the way, use ``tools'' to connect to other resources such as databases, computational units and and actuators.
\end{itemize}

In contrast, a conventional LLM-integrated application without agentic AI relies on humans or pre-written deterministic code to perform all of the above tasks. From this perspective, agentic AI expands the use of LLM agents and further reduces human intervention and increases the level of automation. A comparison is illustrated in Fig.~\ref{fig:agenticwf1}. In the conventional architecture shown in (a), the workflow is either hard-coded or defined externally, and the LLM merely serves as a problem solver. In the agentic AI architecture shown in (b), the LLM autonomously orchestrates the task, determines the workflow, and governs the solution process.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{./chapters/part-4/figures/agenticaiworkflow.png}
	\caption{Conventional architecture (a) versus agentic AI architecture (b).}
	\label{fig:agenticwf1}
\end{figure}

Another commonly used architecture is shown in Fig.~\ref{fig:agenticwf2}. In this design, one LLM generates an output while another LLM evaluates its quality, decides whether to accept or reject it, and provides revision instructions if necessary. The two LLMs collaborate in an iterative loop, refining the solution step by step. This self-evaluative architecture has been shown to significantly reduce the likelihood of low-quality or hallucinated outputs.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\textwidth]{./chapters/part-4/figures/agenticaiworkflow2.png}
	\caption{Self-evaluative agentic AI architecture.}
	\label{fig:agenticwf2}
\end{figure}

LLM agents in an agentic AI system do not rely solely on the internal knowledge encoded in the model. Instead, they can be configured to interact with external tools such as databases, web browsers, calculators, etc., to enhance their problem-solving capabilities. This tool-augmented reasoning is essential for tasks that require up-to-date information, precise computation, or direct interaction with external environments. It also helps with reducing hallucination. A representative architecture is given in Fig.~\ref{fig:agenticwf3}, where the LLM autonomously decides when to invoke an external tool and generates the appropriate commands using pre-defined protocols known as the \myabb{Model Context Protocol}{MCP}. MCP is introduced in Section~\ref{sec:mcp}.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.8\textwidth]{./chapters/part-4/figures/agenticaiworkflow3.png}
	\caption{LLM with memory and tool interfaces to databases, web browsers, calculators, sensors and actuators, and other components that can interact with the environment.}
	\label{fig:agenticwf3}
\end{figure}

Recall RAG which focuses on database-integrated LLMs to expand knowledge boundaries and suppress hallucinations. A fully developed agentic AI can be taken as an extension of RAG. Other than databases, agentic AI systems often adopt memory modules which store historical inputs and outputs to maintain context and support long-term reasoning. This is also shown in Fig.~\ref{fig:agenticwf3}.

\section{Agentic AI Framework} \label{sec:agentic_ai_frameworks}

An \mync{agentic AI framework} refers to the infrastructure that supports the creation, configuration, and execution of an agentic AI system. It provides the necessary abstractions and utilities that allow users to define and deploy agentic AI pipelines more easily. While it is always possible to manually coordinate LLMs, prompts, and tool calls, using a mature agentic AI framework significantly simplifies the process of building scalable, multi-agent, or tool-augmented systems. In this sense, an agentic AI framework plays a role analogous to that of Kubernetes in the context of containerized applications. It abstracts away low-level orchestration logic and enables the declarative specification and runtime management of complex, distributed components. 

On one hand, agentic AI frameworks simplify the deployment of LLM applications, but on the other hand they add additional layers of abstraction and prompts, thus making the system behavior less controllable and sometimes more difficult to debug. The user should choose whether to use agentic AI frameworks wisely based on the need.

Common examples of agentic AI frameworks include the OpenAI Agents SDK, CrewAI, LangGraph, AutoGen, and many more. Details are introduced in later sections.

\subsection{Asynchronous Python}

Before introducing different agentic AI frameworks, it is worth reviewing asynchronous Python programming, as it is widely used across many agentic AI platforms.

Asynchronous programming (often abbreviated as async programming) is a well-established concept that predates agentic AI. In a nutshell, it creates a ``multi-thread-like'' framework where functions do not necessarily run in the order they are defined, but instead follow a ``first-ready, first-served'' execution model. In the context of async Python programming, an event loop switches tasks (among a group of tasks) whenever one is waiting, typically due to I/O. 

It is multi-thread-like in behavior, but there is no true CPU-level parallelism and no true multi-threading. By itself, \verb|asyncio| runs on a single thread and a single CPU core. While async Python programming does not help with reducing the total CPU time consumption, it saves overlapped waiting time of the tasks. Even before the emergence of agentic AI frameworks, async programming had already proven valuable in many applications, such as web servers handling HTTP requests or user interfaces monitoring human actions on dashboards. 

The \verb|asyncio| library is the standard Python library for writing concurrent code using the \verb|async|/\verb|await| syntax, and many other async libraries are built on top of it. A detailed introduction to \verb|asyncio| can be found at \cite{python2025asyncio}. A brief review of the basic syntax is given below.

The basic syntax to define and run a Python \mync{coroutine function} is as follows:
\begin{lstlisting}
import asyncio

async def <coroutine_function_name>(input)[ -> output]:
    <do something; can contain nested coroutines>
    [return <something>]
    
[result = ]asyncio.run(<coroutine_function_name>(input))
\end{lstlisting}
Here, \texttt{async def} defines a coroutine function. Calling a coroutine function directly does not run it immediately. Instead, it returns a coroutine object. The syntax is given below.
\begin{lstlisting}
[<coroutine_object> = ]<coroutine_function_name>(input) # does not execute
\end{lstlisting}
This coroutine object can then be scheduled and executed as follows.
\begin{lstlisting}
[result = ]asyncio.run(<coroutine_object>)
\end{lstlisting}
When executing a coroutine object like this, there is no need to pass its input arguments again. A coroutine object can be executed only once. The function \texttt{asyncio.run()} runs the given coroutine object as the main entry point of the program. It creates and manages the event loop, runs the coroutine until it is complete, and then closes the event loop

Another way to execute a coroutine is with the \verb|await| expression:
\begin{lstlisting}
[result = ]await <coroutine_function_name>(input)
\end{lstlisting}
However, this syntax can only be used inside another coroutine. It cannot be used at the top level, because \verb|await| requires an existing event loop. What it does is suspend the current coroutine until the awaited coroutine completes, allowing the event loop to run other tasks in the meantime.

The \texttt{asyncio.gather()} function is another way to schedule multiple coroutines concurrently on the same event loop. An example is given below. Assume that there is already an event loop. The following syntax
\begin{lstlisting}
x = await cx()
y = await cy()
\end{lstlisting}
will execute them in sequential manner, not the parallel-like manner. This is because \verb|await| behaves like ``there is a coroutine function running and let us pause here until it finishes.'' If \verb|cx| is delayed due to I/O, \verb|cy| must wait until \verb|cx| finishes. There are at least two ways to let the tasks run in the parallel-like manner. Consider
\begin{lstlisting}
x, y = await asyncio.gather(
    cx(), 
    cy()
)
\end{lstlisting}
This way, \verb|cx| and \verb|cy| are scheduled concurrently. While \verb|cx| is waiting for inputs, \verb|cy| can still execute. 

Alternatively, we can also ``register'' the coroutine functions as background tasks.
\begin{lstlisting}
x_task = asyncio.create_task(cx())
y_task = asyncio.create_task(cy())
x = await x_task
y = await y_task
\end{lstlisting}
Tasks are treated differently than coroutine functions. In the above example, when \verb|x_task| is await, the event loop will look for other pending tasks (not coroutine functions) and as a result, \verb|y_task| will be executed. This is what \verb|await| does in general. It pauses the script for the coroutine function or task, and while waiting, it checks other registered tasks and executes those that can run. The same principle applies to the entire event loop. If the whole event loop is paused (its ready queue is empty), it uses the OS-level selector to wait for I/O readiness, and the operating system schedules other work until new events are ready.

The main program can be wrapped as follows so that .
\begin{lstlisting}
import asyncio

async def main():
	<the code to be introduced>
	
if __name__ == "__main__":
	asyncio.run(main())
\end{lstlisting}

\subsection{OpenAI Agents SDK} \label{sec:openaisgentssdk}

OpenAI Agents SDK simplifies the deployment of an agentic AI system without sacrificing flexibility. In fact, it enables more versatile agentic AI pipelines, such as chaining multiple AI agents using handoffs and nesting AI agents within tools-capabilities that would be almost impossible to implement manually due to the complexity of such systems.

Comparing with manually deployment, the OpenAI Agents SDK offers at least the following additional features.

\begin{itemize}
	\item Simplified deployment of a single AI agent or tool
	
	Without an agentic AI framework, the user must create a detailed ``user manual'' in JSON object format and pass it to the LLM. With the OpenAI Agents SDK, the user can simply add a decorator to a Python function to turn it into a tool, without needing to write a separate descriptive JSON object. OpenAI Agents SDK will try to interpret the functionality and the input and output formats of the tool without additional help.
	
	\item Easy conversion of AI agents into tools
	
	An AI agent can itself be converted into a tool with a single-line command. This significantly increases the capabilities of higher-level AI agents that use these tools.
	
	\item Chaining AI agents using handoffs. 
	
	Information processed by an upstream AI agent can be passed to a downstream agent easily via \mync{handoffs}, a mechanism for explicitly transferring control and data between agents during execution. The upstream agent decides when and where to pass the data. This allows true cooperation between AI agents and enables highly flexible agentic AI pipelines.
\end{itemize}

Figure \ref{fig:openai_agents_sdk_pipeline} illustrates the key features of OpenAI Agents SDK compared with a manual implementation. In the OpenAI Agents SDK-based pipeline, an AI agent can be converted into a tool and respond to tool calls (red arrows). Multiple AI agents can also be chained to form an upstream–downstream pipeline via handoffs (blue arrows). When an AI agent has multiple possible downstream agents, it can decide at runtime which one to hand over data and control to.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=\textwidth]{./chapters/part-4/figures/openai_agents_sdk_pipeline.png}
	\caption{Manually implemented (a) versus OpenAI Agents SDK–based (b) agentic AI pipelines. Red arrows represent tool calls, and blue arrows represent handoffs.}
	\label{fig:openai_agents_sdk_pipeline}
\end{figure}

The basic syntax of creating an OpenAI Agents SDK framework based agentic AI system is introduced in the rest of this section. All code examples are assumed to be executed in an environment with a running event loop. Note that \verb|OPENAI_API_KEY| is also assumed to be loaded as an environment variable.

\vspace{0.1in}
\noindent \textbf{Agent}
\vspace{0.1in}

The following example demonstrates how to create an agent with a system prompt and a user prompt.

\begin{lstlisting}
from agents import Agent, Runner, trace, function_tool

system_prompt = "<system prompt>"

agent_instance = Agent(
    name="<agent name>",
    instructions=system_prompt,
    model="<model>"
)
response = Runner.run(<agent name>, "<user prompt>")
output = response.final_output
\end{lstlisting}

The following code is useful when running multiple agents concurrently:
\begin{lstlisting}
with trace("<trace name>"):
    results = await asyncio.gather(
        Runner.run(<agent name 1>, "user prompt 1"),
        Runner.run(<agent name 2>, "user prompt 2"),
        Runner.run(<agent name 3>, "user prompt 3"),
    )
outputs = [result.final_output for result in results]
\end{lstlisting}

The call to \texttt{await asyncio.gather()} works here because an event loop wrapper is already assumed. The function \texttt{asyncio.gather()} schedules all provided coroutines to run concurrently. Notice that the results will be returned in the same order as the input arguments regardless of which coroutine completes first. 

The \verb|trace()| context manager allows the user to monitor all LLM-related calls made to OpenAI, and these traces can be viewed on the OpenAI dashboard.

\vspace{0.1in}
\noindent \textbf{Tool}
\vspace{0.1in}

There are at least two ways to define a tool for an agent, as shown below.

\begin{lstlisting}
@function_tool
def <tool 1>(<input 1>: <input type 1>, <input 2>: <input type 2>, ...):
    """<description>"""
    <do something>
    return {"status": "success", <other returns>}

<tool 2> = <low tier agent name>.as_tool(
    tool_name="<tool name>",
    tool_description="tool description"
)

tools = [<tool 1>, <tool 2>]

agent = Agent(
    name="<high tier agent name>",
    instructions="<system prompt>",
    tools=tools,
    model="<model>"
)
\end{lstlisting}

The first method uses the \verb|@function_tool| decorator and is intended for precise tools such as calculators or database query functions. It requires an explicit list of typed input arguments and returns well-defined, structured results.

The second method uses another low-tier agent as a smart tool via \verb|as_tool|. The input to this tool is simply a string, which is treated as the user prompt for the low-tier agent. This creates a nested structure. The low-tier agent can call its own precise or smart tools, enabling more flexible and modular agentic AI pipelines.

\vspace{0.1in}
\noindent \textbf{Handoff}
\vspace{0.1in}

Handoffs are used to transfer data between agents. In this context, a handoff allows data generated (as well as the entire context to generate the content) by an upstream agent to be passed to a downstream agent. Downstream agents are registered as available handoffs, and the upstream agent can decide when and where to pass the data.

For a downstream agent, use \verb|handoff_description| as a ``self introduction'' which will be presented to the upstream agent. An example is shown below:
\begin{lstlisting}
downstream_agent = Agent(
    name="<name>",
    instructions="<instructions>",
    tools=[<tool 1>, <tool 2>, ...],
    model="<model>",
    handoff_description="<handoff description>"
)
\end{lstlisting}

List all available downstream agents and provide this list to an upstream agent. The upstream agent can then decide at runtime when to pass data and which downstream agent to use.
\begin{lstlisting}
handoffs = [<downstream_agent 1>, <downstream_agent 2>, ...]
upstream_agent = Agent(
    name="<name>",
    instructions="<instructions>",
    tools=[<tool 1>, <tool 2>, ...],
    model="<model>",
    handoffs=handoffs
)
\end{lstlisting}

It is recommended to include clear instructions in the \verb|instructions| (system prompt) of the upstream agent on how and when to use the downstream agents.

Run the upstream agent, and it will automatically trigger the downstream agents if it decides to do so based on its instructions and reasoning. The return value will be the final result of the entire pipeline. Note that handoffs do not force an upstream agent to pass data. It is possible for the upstream agent to return its response directly to the user, completely bypassing the downstream pipeline. In this sense, an upstream agent determines the execution path of the pipeline from itself onward.

\vspace{0.1in}
\noindent \textbf{Guardrail}
\vspace{0.1in}

It is recommended to double-check both the initial user input and the final output of an agentic AI system to ensure that the input meets the required criteria or contains the necessary information, and that the output does not include harmful content.

There are several ways to achieve this. For example, OpenAI provides a moderation API that checks whether a piece of multimodal content contains material related to sexual content, hate, self-harm, and other restricted categories. More about this API will be introduced later in this section, and it will be tested in the project examples.

In the OpenAI Agents SDK framework, guardrails can be implemented by defining dedicated AI agents to check the input and output of the agentic AI system according to customized requirements. Such guardrail agents return a boolean flag indicating whether the guardrail was triggered, along with details about what content caused the trigger. If a guardrail is triggered, it will raise an exception.

The following syntax demonstrates the minimum implementation of a guardrail. It involves the following steps.
\begin{enumerate}
	\item Define a class to hold the return value of the guardrail agent. 
	\item Define the guardrail agent itself.  
	\item Define a coroutine function, decorated with \verb|@input_guardrail| or \verb|@output_guardrail|, that triggers the guardrail agent.
\end{enumerate}

\begin{lstlisting}
from pydantic import BaseModel
from openai_agents import Agent, Runner, input_guardrail, output_guardrail, GuardrailFunctionOutput


class <guardrail return class>(BaseModel):
	<is_triggered>: bool
	<reason or content>: str

guardrail_agent = Agent(
	name="<guardrail check agent name>",
	instructions="<instructions, often the things to check>",
	output_type=<guardrail return class>,
)

@output_guardrail
async def <guardrail coroutine name>(ctx: RunContextWrapper, agent: Agent, output: MessageOutput) -> GuardrailFunctionOutput:
	result = await Runner.run(guardrail_agent, output.response, context=ctx.context)
	return GuardrailFunctionOutput(output_info=result.final_output,tripwire_triggered=result.final_output.<is_triggered>)
\end{lstlisting}

When defining an agent in the pipeline, include the above coroutine function as a guardrail:

\begin{lstlisting}
agent = Agent( 
	name="Customer support agent",
	instructions="<instructions>",
	output_guardrails=[<guardrail coroutine name>],
)
\end{lstlisting}

The above example defines an output guardrail. An input guardrail can be defined in a similar way.

Notice that in OpenAI Agents SDK, \verb|input_guardrails| only run when attached to the \emph{first} agent in a chain (on the initial user input), and \verb|output_guardrails| only run when attached to the \emph{last} agent (on the final agent output). Guardrails attached to intermediate agents do not trigger.

Recall that OpenAI provides moderate API to check whether a content contains harmful information. It can be used as the output guardrail as follows.
\begin{lstlisting}
@output_guardrail
async def moderation_guardrail(ctx, agent: Agent, message):
	response = client.moderations.create(
		model="omni-moderation-latest",
		input=message.response
	)
	flagged = response.results[0].flagged
	return GuardrailFunctionOutput(output_info=response.results[0],tripwire_triggered=flagged)
\end{lstlisting}

An example using OpenAI Agents ADK is given in Section \ref{sec:agentaiexp-agentsdk}.

\subsection{CrewAI}

CrewAI is not just a framework but also a company and an ecosystem. Depending on the context, CrewAI can refer to the following:
\begin{itemize}
	\item A company that develops and provides agentic AI solutions.
	\item A platform offered by CrewAI to deploy agentic AI systems on the cloud.
	\item An application with a graphical interface that allows users to design and deploy agentic AI pipelines with minimal coding.
	\item An open-source agentic AI framework.
\end{itemize}
In this notebook, we will focus on the CrewAI framework.

Recall the OpenAI Agents SDK introduced in Section~\ref{sec:openaisgentssdk} and Fig.~\ref{fig:openai_agents_sdk_pipeline}, where the user defines agents, tools, and handoffs, and links them together, while the agents decide which tools and handoffs to use. The CrewAI framework follows a very different philosophy. A ``Crew'' in this context is a team (e.g., engineering, data science, or marketing), where each AI agent acts as an employee. A job is assigned to the team, with specified context, inputs, and outputs. The user plays the role of a ``hiring manager'', defining the roles and characteristics of each AI agent, the context and goals of each task, and assembling the team from scratch. Once all agents and tasks are defined, the team completes the job. This is demonstrated by Fig. \ref{fig:crewai_pipeline}. Notice that a ``manager'' agent can be assigned to a crew to determine the pipeline flow.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.8\textwidth]{./chapters/part-4/figures/crewai.png}
	\caption{CrewAI framework pipeline, where the user defines agents, tasks and their associations and the crew get things done automatically.}
	\label{fig:crewai_pipeline}
\end{figure}

In this sense, while the OpenAI Agents SDK requires the user to micro-manage pipeline details, CrewAI emphasizes macro-management by defining context, inputs and outputs, and assigning agents to tasks. The OpenAI Agents SDK is more flexible and feel more under control, but when a job aligns well with CrewAI's philosophy, implementing it in CrewAI is often much easier. Another interpretation is that CrewAI can be used to build up complicated applications in an easier manner, but it may lack precision.

\vspace{0.1in}
\noindent \textbf{CrewAI Installation as a UV Tool}
\vspace{0.1in}

CrewAI provides both Python libraries and CLI tools. It is recommended to install both when working with CrewAI. The CLI tool simplifies project setup by generating a directory tree with template Python scripts that can be adapted for specific applications. Manually creating such a structure from scratch would be tedious.

To install or update CrewAI as a tool with \verb|uv|, use:
\begin{lstlisting}
uv tool install crewai
uv tool install crewai --upgrade
\end{lstlisting}

Once CrewAI is installed, a new project can be created with:
\begin{lstlisting}
crewai create crew <project_name>
\end{lstlisting}

This command generates a project root folder containing a hello-world-style template. During creation, the user is prompted to select a default LLM service provider and model, and to provide an API key. These choices are used in generating the template but can be modified later.

An example of the template directory structure is shown below \cite{crewai2025install}:
\begin{lstlisting}
my_project/
|-- .gitignore
|-- knowledge/
|-- pyproject.toml
|-- README.md
|-- tests/
|-- .env
\-- src/
  \-- my_project/
    |-- __init__.py
    |-- main.py
    |-- crew.py
    |-- tools/
    |  |-- custom_tool.py
    |  \-- __init__.py
    \-- config/
      |-- agents.yaml
      \-- tasks.yaml
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Agents and Tasks Definition}
\vspace{0.1in}

In the project template, \verb|agents.yaml| and \verb|tasks.yaml| are configuration files that specify the properties of agents and tasks. An agent refers to an LLM with defined capabilities and tools, while a task represents a workflow step with input, desired output, and context. These configuration files may include prompts, parameters, and other metadata that shape the agent’s behavior and the execution of tasks.

For example, in a document-refinement workflow, the agent might be a ``publication editor,'' and the task could be ``correct-grammar-error.'' The input would be text containing grammatical mistakes, and the output would be error-free text that preserves the original meaning with minimal changes.

Below is an example of \verb|agents.yaml| created during project initialization:
\begin{lstlisting}
researcher:
  role: >
    {topic} Senior Data Researcher
  goal: >
    Uncover cutting-edge developments in {topic}
  backstory: >
    You're a seasoned researcher with a knack for uncovering the latest developments in {topic}. Known for your ability to find the most relevant information and present it in a clear and concise manner.

reporting_analyst:
  role: >
    {topic} Reporting Analyst
  goal: >
    Create detailed reports based on {topic} data analysis and research findings
  backstory: >
    You're a meticulous analyst with a keen eye for detail. You're known for your ability to turn complex data into clear and concise reports, making it easy for others to understand and act on the information you provide.
\end{lstlisting}

Here, two agents, ``researcher'' and ``reporting analyst,'' are defined. Each agent has three key components: \verb|role|, \verb|goal|, and \verb|backstory|. Together, these specify the agent’s intended capabilities and behavior. The symbol \verb|>| in YAML indicates a folded block scalar, meaning that multi-line text is treated as a single string with line breaks converted to spaces. The placeholder \verb|{topic}| in the example will be filled dynamically by the higher-level script.

In addition to the compulsory \verb|role|, \verb|goal|, and \verb|backstory| fields, other optional fields may be included. A commonly used one is \verb|llm|, which lets the user specify an LLM model other than the default. For example:
\begin{lstlisting}
<agent_name>:
  role: <something>
  goal: <something>
  backstory: <something>
  llm: openai/gpt-5-mini
\end{lstlisting}
A full list of allowed fields can be found at \cite{crewaiAgentsConcepts}, including tools, delegation, maximum iteration count, rate limits, and others.

Below is an example of \verb|tasks.yaml| created during project initialization:
\begin{lstlisting}
research_task:
  description: >
    Conduct thorough research about {topic}. Make sure you find any interesting and relevant information given the current year is {current_year}.
  expected_output: >
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: >
    Review the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains all relevant information.
  expected_output: >
    A fully fledged report with the main topics, each with a full section of information. Formatted as markdown without '```'
  agent: reporting_analyst
\end{lstlisting}

Here, two tasks are defined, each assigned to an agent. The user provides a description and expected output for each task. In addition to \verb|description| and \verb|expected_output|, other optional fields can also be specified. A full list can be found at \cite{crewai2025tasks}, including settings that limit the tools available for a task, enforce output formats, and define guardrails.

\vspace{0.1in}
\noindent \textbf{Crew Definition}
\vspace{0.1in}

A crew is an integration of agents and tasks defined in the YAML files. The configuration files define the ``class'' of different types of agents and tasks, while the crew Python script reads these configurations and instantiates them. An example is given below. Notice that different decorators are used.
\begin{lstlisting}
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class <crew class name>():
"""<description of the crew>"""

  agents: List[BaseAgent]
  tasks: List[Task]

  @agent
  def researcher(self) -> Agent:
    return Agent(
      config=self.agents_config['researcher'],
      verbose=True
    )

  @agent
  def reporting_analyst(self) -> Agent:
    return Agent(
      config=self.agents_config['reporting_analyst'],
      verbose=True
    )

  @task
  def research_task(self) -> Task:
    return Task(
      config=self.tasks_config['research_task'],
    )

  @task
  def reporting_task(self) -> Task:
    return Task(
      config=self.tasks_config['reporting_task'],
      output_file='report.md'
    )

  @crew
  def crew(self) -> Crew:
  """<description>"""
    return Crew(
      agents=self.agents,
      tasks=self.tasks,
      process=Process.sequential,
      verbose=True,
    )
\end{lstlisting}

When using the \verb|agent| and \verb|task| decorators, the defined components are automatically added to the \verb|self.agents| and \verb|self.tasks| lists in their defined order. Notice that in this example \verb|Process.sequential| is used. In this case, the sequence of tasks in \verb|self.tasks| matters, as this will be the task pipeline of the crew. In this sense, the user defines the pipeline.

It is possible to assign a \mync{manager agent} to supervise the crew and ensure the quality of generations. In this setup, the manager oversees the behavior of each agent and task, and can determine the crew’s execution flow. A manager agent can be assigned in the \verb|@crew| definition as follows:
\begin{lstlisting}
@crew
def crew(self) -> Crew:
"""<description>"""
  
  manager = Agent(
    config=self.agents_config['manager'],
    allow_delegation=True
  )
  
  return Crew(
    agents=self.agents,
    tasks=self.tasks,
    process=Process.hierarchical,
    verbose=True,
    manager_agent=manager,
  )
\end{lstlisting}

Here, a manager is defined, assuming its role description is provided in the \verb|agents| configuration file. It is then assigned to the crew via \verb|manager_agent|. The process is changed from \verb|Process.sequential| to \verb|Process.hierarchical|, allowing the manager to supervise the crew above other agents. The manager can dynamically re-order, route, or request follow-ups across tasks and agents. In hierarchical mode, the pipeline is determined not by the sequence of task definitions but by the manager agent.

It is also possible to assign an LLM model directly as the manager without creating an agent instance or defining it in the \verb|agents| configuration file. However, it has been reported that a properly defined manager agent may outperform a bare model assignment.

It is possible to enforce structured output for a task. This can be done in a manner similar to the OpenAI Agents SDK. A Pydantic model inheriting from \verb|BaseModel| is defined with the desired output structure. An example is shown below:
\begin{lstlisting}
class <ModelName>(BaseModel):
    """ <description> """
    <field>: <type> = Field(description="<description>")
    <field>: <type> = Field(description="<description>")
    <field>: <type> = Field(description="<description>")
\end{lstlisting}
When defining a task, the model is specified via the \verb|output_pydantic| argument:
\begin{lstlisting}
@task
def <task_name>(self) -> Task:
    return Task(
        config=self.tasks_config['<task name in configuration>'],
        output_pydantic=<ModelName>,
    )
\end{lstlisting}
Notice that Pydantic models can be nested. For example, one can define a model with a field that is a list of another Pydantic model.

It is not required but can become helpful sometimes to also remind CrewAI of the required structured output in the configuration files for tasks and agents.

\vspace{0.1in}
\noindent \textbf{Execution}
\vspace{0.1in}

Finally, the crew can be executed as follows:
\begin{lstlisting}
def run():
"""
Run the crew.
"""
  inputs = {
	'topic': 'AI LLMs',
	'current_year': str(datetime.now().year)
  }

  try:
    DocumentRefiner().crew().kickoff(inputs=inputs)
  except Exception as e:
    raise Exception(f"An error occurred while running the crew: {e}")

if __name__ == "__main__":
	run()
\end{lstlisting}

The above summarizes the basic usage of the CrewAI framework. More advanced materials are introduced in the following sections.

\vspace{0.1in}
\noindent \textbf{Tool}
\vspace{0.1in}

Tools are one of the most important features of an agentic AI system, as they significantly extend its capabilities. Recall that the OpenAI Agents SDK provides several ways for a user to define tools, including at least the following:
\begin{itemize}
	\item Using tools from a library, for example, \verb|WebSearchTool| provided by OpenAI, which allows an LLM to query information from the web.
	\item Defining a custom function tool with the \verb|@function_tool| decorator.
	\item Supplying a JSON-formatted descriptive tool.
	\item Using an agent as a tool.
\end{itemize}

CrewAI likewise provides multiple methods for calling tools from libraries or defining customized tools. More details are given below.

User defined tools need to be saved under the folder \texttt{tools} as a function. An example is given below.
\begin{lstlisting}
@tool("<tool name>")
def <tool function name>(<input>) -> str:
    <do something>
    ...
    try:
        <do something>
        return "Tool executed successfully"
    except Exception as e:
        return f"An error occurred: {e}"
\end{lstlisting}

In the \verb|crew.py| file, import tools as follows.
\begin{lstlisting}
from crewai_tools import <tool function name> # official tool 
from <project name>.tools.<tool file name> import <tool function name> # user-defined tool
\end{lstlisting}
When defining an agent, assign tools to it as follows.
\begin{lstlisting}
@agent
def <agent name>(self) -> Agent:
    return Agent(
        config=self.agents_config['<agent name>'],
        tools=[<tool function name>, <tool function name>],
        verbose=True,
        allow_delegation=True
    )
\end{lstlisting}

Do not forget to introduce the tool to the agent in \verb|agents.yaml|. When introducing the tool, use \texttt{<tool name>} in the decorator.

\vspace{0.1in}
\noindent \textbf{Memory}
\vspace{0.1in}

LLMs are inherently stateless. In an LLM-based chatbot application, the conversation between the user and the model is typically stored and then reused as context whenever the LLM is triggered. Conventionally, the conversation is saved in a JSON file, for example \verb|history.json|, which may look like:
\begin{lstlisting}
[
	{"role": "user", "message": "<something>"},
	{"role": "assistant", "message": "<something>"},
	...
]
\end{lstlisting}

When the conversation history grows long, feeding the entire record to the LLM for every call is problematic for at least the following reasons.
\begin{itemize}
	\item Much of the historical dialogue may be irrelevant to the current query, making it wasteful to process.
	\item The total token count of the history may exceed the LLM’s input limit.
\end{itemize}
This naturally motivates several strategies as follows.
\begin{itemize}
	\item Query only for relevant past information before sending it as context to the LLM, rather than including the full history.
	\item Periodically prune older conversations, as they tend to become less relevant over time.
	\item Selectively preserve information likely to be useful in the future by tagging it so that it is not removed.
\end{itemize}
These ideas form the foundation of memory management in CrewAI.

As an agentic AI framework, CrewAI involves multiple LLM agents. Memory is managed at the agent level, the task level and the crew level, and memory sharing across agents can be enabled if desired. CrewAI defines several memory types, such as short-term memory, long-term memory, and entity memory, and each can be activated independently. When a memory type is enabled, CrewAI automatically promotes selected conversations into that memory category. Unlike conventional conversation logs stored solely in JSON format, CrewAI combines JSON (for raw text), embedded vectors (for semantic retrieval), and SQLite (for efficient tagging and metadata queries) to store memory. The storage medium depends on the memory type.

The commonly used memory types and their features are introduced below.

Short-term memory. Short-term memory stores the recent inputs and outputs of an AI agent using embedded vectors. It relies on ChromaDB \cite{chroma2025chroma}, an open-source, file-based embedding database.

Long-term memory. Long-term memory stores important task results that can be reused across sessions. Both ChromaDB and SQLite are employed for this purpose.

Entity memory. Entity memory tracks entities such as names, places, and concepts. It also uses ChromaDB for storage.

The above three memory types are collectively referred to as the basic memory system, and they can be enabled by:
\begin{lstlisting}
crew = Crew(
    agents=[...],
    tasks=[...],
    process=Process.sequential,
    memory=True,  # Enables short-term, long-term, and entity memory
    verbose=True
)
\end{lstlisting}

The user can choose the storage location and the vector embedding tool. If not specified, a default location inside the project folder is used, and the OpenAI embedding API is applied. The behavior of the embedding tool can also be configured. More details are given at \cite{crewai2025memory}.

Once memory is properly enabled, the overall performance of the agentic system is improved. In general, it becomes more efficient, potentially cheaper—since semantic search is used instead of retrieving the entire chat history as context—and it remembers important concepts more accurately and consistently. Applications benefit from these improvements naturally, without needing to manage the internal details of the memory system.

\vspace{0.1in}
\noindent \textbf{Coder Agent}
\vspace{0.1in}

On a machine where containerization tools are installed, an agent can trigger the execution of a code that it generates, and it refines the code based on the error message or output or several times. This improves the stability and performance of a coder agent.

The user is able to setup maximum trail number and maximum waiting time for the trail code execution.

\subsection{LangGraph}

It is worth mentioning the differences between LangChain and LangGraph. They are two distinct products from the same company, LangChain. A brief explanation is given below.

\mync{LangChain} is an open-source, composable framework that provides a standard interface for LLM models and APIs to databases and a variety of tools \cite{langchain2025}. Suppose that an agentic AI system with multiple components has been defined. If LangChain is used to connect and integrate these components, the user can switch underlying models (for example, from OpenAI to Anthropic) and tools (for example, from MySQL to PostgreSQL) with relatively little code change. This greatly enhances the portability and robustness of the agentic AI solution.

From that sense, LangChain is to some extend similar with MCP, but in the LangChain ecosystem. Nowadays, some agentic AI frameworks developed by other entities, such as AutoGen, also support connectivity with LangChain.

\mync{LangGraph}, on the other hand, is an agentic AI framework, broadly comparable to the OpenAI Agents SDK and CrewAI. It focuses on coordinating AI agents and reliably handling complex tasks by representing applications as stateful graphs \cite{langchain2025langgraph}. It is sufficient to use only LangChain to build an LLM application with deterministic pipeline. LangGraph, on the other hand, can be taken as an abstraction layer built on top of LangChain that provides orchestration intelligence, enriching the system with agentic AI capability.

This section focuses on LangGraph, the agentic AI framework. Technologies offered by LangChain will also be introduced and used. After all, LangGraph uses LangChain under the hood.

Install LangGraph as a Python package as follows.
\begin{lstlisting}
uv add langgraph
\end{lstlisting}
or
\begin{lstlisting}
uv pip install -U langgraph
\end{lstlisting}

LangGraph introduces the concepts of states, nodes and edges to visualize the workflow as a graph. A \mync{LangGraph state} represents a snapshot of the application. The state is immutable, meaning that each update of any kind creates a new state derived from the old one. In this way, LangGraph maintains a full history of the system status and supports time-travel debugging and replay. A \mync{LangGraph node} typically represents an atomic unit of computation, such as a function, tool, or an agent call. It receives the current state as input and produces a new state derived from it, with some data processed. Finally, a \mync{LangGraph edge} represents a directed route that connects nodes, thereby defining the pipeline of the data flow.

An example is given in Fig. \ref{fig:langgraph_simpledemo} to demonstrate the concept of nodes and edges. In the figure, apart from the start and end nodes, two nodes are defined, namely the LLM chatbot node and the tool node, likely for resource augmentation. The arrows are edges, where the solid arrows represent deterministic paths, and the dashed arrows, conditional paths. When the LLM chatbot node decides to use a tool call, tool node is executed. The result is returned to the LLM chatbot node. This process is iteratively carried out until the LLM node decides to proceed without tool calls. Along the edges, states are passed from one node to the other.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\textwidth]{./chapters/part-4/figures/langgraph_simpledemo.png}
	\caption{A simple demonstration of LangGraph with one LLM and one tool.}
	\label{fig:langgraph_simpledemo}
\end{figure}

The following steps are typically required to define a LangGraph-based application.
\begin{itemize}
  \item Define the state class, often in the form of a Python dataclass or Pydantic model.
  \item Create a state graph builder with the defined state class.
  \item Create nodes and add nodes to the graph builder.
  \item Create edges and add edges to the graph builder.
  \item Compile the graph builder to obtain the graph.
  \item Invoke the graph with the initial state.
\end{itemize}

Detailed introduction is given below.

\vspace{0.1in}
\noindent \textbf{State Definition}
\vspace{0.1in}

As explained earlier, the state class is essentially a data structure, such as a Pydantic model or a \texttt{TypedDict}. The following is an example from the official documentation.

\begin{lstlisting}
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
\end{lstlisting}

Here, \texttt{TypedDict} is used as the basis of the state class. Pydantic \texttt{BaseModel} is also supported, but it is generally less performant than \texttt{TypedDict} \cite{langgraph2025compilingYourGraph}. In the rest of this chapter, \texttt{TypedDict} will be used for LangGraph.

It is worth mentioning the \mync{reducer function}, which can be defined in a state. As noted earlier, a node creates and outputs a new state based on the existing state it receives as input. By default, a node may overwrite fields in the new state with fresh values, discarding the previous contents of those fields. This does not affect the connectivity of the graph itself, but it may result in the loss of useful historical information. 

A reducer function provides a mechanism to preserve such information. If a field in the state is associated with a reducer function, the node must update the field through that function. The reducer function specifies how new values should be merged with existing ones, ensuring that past information is retained. For example, a common reducer function appends new values to a list so that all historical entries for the field are preserved.

There are different ways to define a reducer, one of which is through Annotated. \mync{Python Annotated} allows the programmer to attach metadata to a variable, often specifying its data type and usage. In the example above, \texttt{add} from the \texttt{operator} library is used as the annotation. This reducer function specifies how \verb|bar| should be updated. In this case, new values should be concatenated to the list.

LangGraph provides commonly used state classes and reducer functions that can be imported and used directly, many of which in \texttt{langgraph.graph}.

Once the state is defined, a \mync{state graph builder} can be declared as follows. All the nodes and edges what will be added to this graph later will use the defined state in the data pipeline.
\begin{lstlisting}
class State(TypedDict):
    <defined earlier>

from langgraph.graph import StateGraph

builder = StateGraph(State)
\end{lstlisting}

\begin{mdframed}
\noindent \textbf{Can I use any field name for any purpose?}

For most field names, yes. The user has the flexibility to define any field names. In the custom nodes, the user can add any contents to these fields as part of the pipeline.

However, there are so called ``system-managed'' fields. Some system level functions will try to use particular field names for communication. An example is the \verb|messages| field, which is used for AI agents with LLM to communicate with \verb|ToolNode|.

For that reason, \verb|messages| field is almost always defined in the state as follows.

\begin{lstlisting}
messages: Annotated[list[str], add_message]
\end{lstlisting}

\end{mdframed}

\vspace{0.1in}
\noindent \textbf{Node Definition}
\vspace{0.1in}

A node is a function that consumes the current state and produces a new state. For example, assume the state is defined as follows.
\begin{lstlisting}
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]

from langgraph.graph import StateGraph
builder = StateGraph(State)
\end{lstlisting}

A node can then be defined as:
\begin{lstlisting}
def update_state(state: State) -> State:
    return {
        "foo": state["foo"] + 1,
        "bar": ["new item"]   # appears as an overwrite
    }
\end{lstlisting}
In this example, the node outputs a dictionary. The update to the \texttt{bar} field appears to be an overwrite. However, since a reducer is defined for \texttt{bar}, LangGraph intercepts the update and applies the reducer instead, resulting in concatenation of the old and new values.

The node can be added to the graph as follows. The function name is passed directly into the \verb|add_node| method without requiring an additional decorator:
\begin{lstlisting}
<builder name>.add_node("<node name>", <node function name>)
\end{lstlisting}
For the earlier example:
\begin{lstlisting}
builder.add_node("first_node", update_state)
\end{lstlisting}

In practice, a node may contain an LLM call. An example is shown below:
\begin{lstlisting}
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list[str], add_messages]
    
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="<model name>")

def update_state(state: State) -> State:
    response = llm.invoke(state["messages"])
    new_state = {
        "messages": [response]
    }
    return new_state
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Edge (and Handoff) Definition}
\vspace{0.1in}

Edges can be either deterministic or optional. 

To add a deterministic edge that connects two nodes, use:
\begin{lstlisting}
<builder name>.add_edge("<upstream node name>", "<downstream node name>")
\end{lstlisting}

In the earlier example, the following edges can be added:
\begin{lstlisting}
from langgraph.graph import START, END

<builder name>.add_edge(START, "first_node")
<builder name>.add_edge("first_node", END)
\end{lstlisting}
Here, \texttt{START} and \texttt{END} represent the default entry and exit points of the graph.

For optional edges, a routing function is used to decide which edge to select.
\begin{lstlisting}
<builder name>.add_conditional_edges("<upstream node>", <routing function>)
\end{lstlisting}
where the return of the routing function needs to be a string of the name of the desired downstream node. Boolean routing function can be used as well, in which case the syntax is given below.
\begin{lstlisting}
<builder name>.add_conditional_edges("<upstream node>", <routing boolean function>, {True: "<downstream node 1>", False: "<downstream node 2>"})
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Graph Compiling and Invoking}
\vspace{0.1in}

Once all nodes and edges have been added, the graph must be compiled before it can be executed:
\begin{lstlisting}
<graph name> = <builder name>.compile()
\end{lstlisting}

After compilation, the graph can be visualized as a Mermaid diagram in a Jupyter notebook:
\begin{lstlisting}
from IPython.display import Image, display

display(Image(<graph name>.get_graph().draw_mermaid_png()))
\end{lstlisting}

To invoke a graph once with an initial state, use the following. This will trigger a \mync{super step} which is the execution of the whole graph.
\begin{lstlisting}
<graph name>.invoke(<initial state>)
\end{lstlisting}
It is also possible to run a invoke a graph in asynchronous mode as follows.
\begin{lstlisting}
<graph name>.ainvoke(<initial state>)
\end{lstlisting}
Alternatively, the graph can be executed step by step using \verb|.stream()|, which yields intermediate states and outputs at each stage of the execution.

\begin{mdframed}
\textbf{Does LangGraph necessarily involve LLM?}

No. LangGraph and LangChain can be used to develop applications that have nothing to do with LLMs. Data processing is handled by the nodes. Technically, it is possible to build an application in which all nodes perform conventional computations without involving any LLM.

In an LLM-based application, the state class often contains text, and LLMs are introduced in the nodes to process that text.
\end{mdframed}

Note that memory are not automatically synchronized between different super steps. As a result, the user must either use a global parameter to save the chat history and supply it as the initial state for each super step, or persist the information in a database and let the node retrieve it via tools.

\vspace{0.1in}
\noindent \textbf{Tool}
\vspace{0.1in}

LangGraph allows packaging a function into a tool conveniently. First, define the function you want to package. Then wrap the function with \verb|Tool|. A simple example is shown below:
\begin{lstlisting}
def <function name>(input) -> <output type>:
    <do something>
    return <output>

from langchain.agents import Tool
<tool> = Tool(
    name="<tool name>",
    func=<function name>,
    description="<description>"
)
\end{lstlisting}

To test the tool, use
\begin{lstlisting}
result = <tool>.invoke(<input>)
\end{lstlisting}
which should directly triggers the function defined in the tool.

To run a tool in AI agents, use either
\begin{lstlisting}
result = <tool>.run(<input>)
\end{lstlisting}
or
\begin{lstlisting}
result = await <tool>.arun(<input>)
\end{lstlisting}
to execute it in synchronous or asynchronous mode respectively.

The LangChain community provides a variety of tools out of the box. An example is shown below:
\begin{lstlisting}
from langchain_community.utilities import GoogleSerperAPIWrapper

serper = GoogleSerperAPIWrapper()

tool_search = Tool(
    name="tool_search",
    func=serper.run,  # serper.run() triggers the function
    description="useful for making online queries for additional information"
)
\end{lstlisting}

Recall that a node is essentially a user-defined function. To use tools in a node, simply call the tool functions within the node’s source code.

In an agentic AI application, a node is often an LLM, and it decides what tools to trigger in a dynamic manner. In this case, consider binding the tools to the LLM as follows.
\begin{lstlisting}
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list[str], add_messages]
    
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="<model name>")
llm_with_tools = llm.bind_tools([<tool 1>, <tool 2>, ...])

def update_state(state: State) -> State:
    response = llm_with_tools.invoke(state["messages"])
    new_state = {
        "messages": [response]
    }
    return new_state
\end{lstlisting}
By using \verb|.bind_tools()|, the LLM learns the use of those tools. The user does not need to prepare JSON documents to introduce the tools for the LLM.

Add the set of tools as a ``tool node'' to the graph. Use conditional edges so that the LLM determines whether to call a tool. A typical pattern is as follows.
\begin{lstlisting}
from langgraph.prebuilt import ToolNode, tools_condition

<builder name>.add_node("<tool node>", ToolNode(tools=[<tool 1>, <tool 2>, ...]))
<builder name>.add_conditional_edges("<llm node>", tools_condition, "<tool node>")
\end{lstlisting}
Notice that when using \verb|tools_condition|, if the LLM decides to not use any tool, the edge is routed to \verb|END|. Hence, there is no need to specifically connect the LLM node to the end node.

\vspace{0.1in}
\noindent \textbf{Structured Output}
\vspace{0.1in}

The nodes in a graph take the state as input and output. Therefore, there is no need to structure the output of a graph. When comes to individual LLMs, it is possible to enforce the output as follows.
\begin{lstlisting}
from pydantic import BaseModel, Field

class <pydantic model>(BaseModel):
    <field>:<type> = Field(description="<description>")
    <field>:<type> = Field(description="<description>")
    ...

from langchain_openai import ChatOpenAI

<llm> = ChatOpenAI(model="<model name>")
<llm>.with_structured_output(<pydantic model>)
\end{lstlisting}
where \verb|.with_structured_output()| function is used to associate a Pydantic model with an LLM's output.

\vspace{0.1in}
\noindent \textbf{Memory Sharing across Super Steps}
\vspace{0.1in}

As explained earlier, each invocation of the graph is known as a super step. By default, the input to a super step contains only the initial state. The memory from the previous super step is not automatically carried over to the next one. 

A straightforward way to share memory across super steps is to manually add the conversation or results from earlier steps to the initial state of the next step. While this works, it is tedious and error-prone. LangGraph provides a native and robust mechanism for sharing memory across super steps. A demonstrative example is shown below:
\begin{lstlisting}
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()

<define class, graph builder, add nodes and edges>

graph = builder.compile(checkpointer=memory)
config = {"configurable": {"thread_id": "1"}}
graph.invoke(<initial state>, config=config)
\end{lstlisting}

In the above example, \texttt{MemorySaver} is used to share memory across super steps. When compiling the graph, pass it as the checkpointer. When invoking a super step, set a thread ID in the configuration. All super steps with the same thread ID will share the same memory.

One can use the following command:
\begin{lstlisting}
graph.get_state_history(config)
\end{lstlisting}
to retrieve all checkpointer values, i.e., the complete sequence of state snapshots since the first invocation of the graph. It is also possible to rewind to a specific checkpoint in this history and restart the graph from that point, which can be useful for certain applications.

\subsection{AutoGen}

\mync{AutoGen} is an open-source agentic AI solution offered by Microsoft. Notice that just like LangGraph, AutoGen by itself is not just an agentic AI framework, but more of a comprehensive environment that provides variety of agentic AI applications relevant tools and solutions. As of this writing, it includes at least the following solutions \cite{autogen2025}.

\begin{itemize}
	\item \mync{AutoGen Core}
	
	AutoGen Core refers to a collection of concepts and tools that offer an easy way to quickly build event-driven, distributed, scalable, resilient AI agent systems.
	
	\item \mync{AutoGen AgentChat} 
	
	AutoGen AgentChat is a high-level API for building multi-agent applications. It is built on top of the AutoGen Core package.
	
	\item \mync{AutoGen Extensions}
	
	AutoGen Extensions refers to a collection of APIs, models or tools that extends AutoGen Core's capability. It is often used together with AutoGen Core and AutoGen AgentChat.
	
	\item \mync{AutoGen Studio}
	
	AutoGen Studio is a low-code interface for agentic AI applications prototyping. It is built on top of AutoGen Agent Chat.
	
\end{itemize}

As far as the scope of this notebook concerns, we focus on AutoGen Core, AutoGen AgentChat and AutoGen Extensions.

\vspace{0.1in}
\noindent \textbf{Agent}
\vspace{0.1in}

The minimum realization of an AI agent in AutoGen framework is given below.


\vspace{0.1in}
\noindent \textbf{Agent with Multimodal LLM}
\vspace{0.1in}




\vspace{0.1in}
\noindent \textbf{Tool}
\vspace{0.1in}

AutoGen allows easy definition of a tool in the form of a function. It is also compatible with tools defined by LangChain and MCP. More details of MCP is introduced in the next Section~\ref{sec:mcp}. Syntax and examples are given below.

\vspace{0.1in}
\noindent \textbf{Team}
\vspace{0.1in}

AutoGen allows the user to define a team that consists of multiple AI agents for a task. The role of each AI agent is defined. The overall task is defined. The team tries to achieve the objective by letting the AI agents in the team communicate freely and discuss the assigned task, until the conclusion is drawn. This is to some extent similar with the idea of ``crew'' in CrewAI. 

\section{Model Context Protocol} \label{sec:mcp}

\mync{Model Context Protocol}[MCP] is an open-source standardized protocol to connect AI agents to variety of resources and tools.

In earlier Section \ref{sec:agentic_ai_frameworks}, we have seen how different agentic AI frameworks incorporate tools in their systems. The following is a summary.
\begin{itemize}
	\item Manual deployment
	
	The tools are formed as a Python function. JSON ``user manuals'' are provided to the LLM, with detailed explanation on how to call the function. The LLM agent calls the function when needed.
	
	\item OpenAI Agents SDK
	
	The tools are formed as a Python function. Decorator is used to convert the function into a function tool. A collection of function tools is provided to the LLM. The LLM learns to use the function tools using the function description and no additional user manual is required.
	
	Alternatively, an LLM agent can be converged into a tool with a single-line command.
	
	\item CrewAI
	
	The tools are formed as Python functions and saved at specified locations. Decorator is used. Tool names are provided to agents and tasks to specify what tool they can use.
	
	\item LangGraph
	
	The tools are formed as Python functions. Class \verb|Tool()| or \verb|StructuredTool()| are used to wrap the tools. The available tools information is provided to the LLM using \verb|bind_tools()| function. ToolNodes are used. The LangGraph graph builder automatically route the requests between LLM agents and ToolNodes.
	
	LangGraph has a huge community where people share tools.
\end{itemize}

From the above description, we can see that it is often easy to wrap a function and build a tool and use it privately. However, it is challenging to share tools from different applications due to the different framework requirement and personal programming habits. The LangGraph community has made some progress with how people can share tools that they have developed. However, the usage of these tools is mostly limited to LangGraph users.

MCP is a standard and a solution that tries to make the sharing and recycling of tools and other resources in agentic AI systems easier across applications.  

\subsection{MCP Structure} \label{sec:mcpstructure}

The following concepts are introduced in the context of MCP.
\begin{itemize}
	\item Host: the agentic AI based application.
	\item \mync{MCP client}: the piece of program that runs in the host that provides an interface to the host of an MCP service.
	\item \mync{MCP server}: the software corresponds with MCP client that actually provides the MCP service.
\end{itemize}

Figure \ref{fig:mcpstructure} is used to demonstrate the relationship among host, MCP client and MCP server.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.8\textwidth]{./chapters/part-4/figures/mcpstructure.png}
	\caption{Demonstration of host program, MCP client and MCP server relationship.}
	\label{fig:mcpstructure}
\end{figure}

Some highlights are as follows.
\begin{itemize}
	\item Each MCP client has a corresponding MCP server. 
	\item In most scenarios, MCP servers run on the same physical machine as the host program. It is possible, though rare in practice, that MCP servers run on different machines other than the host program.
	\item The information source for MCP servers does not have to be located on the same physical machine. For example, an MCP server may access information on the Internet or a remote database.
	\item There are options of the connectivity channels between MCP servers and clients. In the figure, \verb|stdio| is used, which is indeed a popular choice.
\end{itemize}

MCP can be used for standardizing and sharing varieties of agentic AI resources. It has been most popular for sharing tools.

\subsection{MCP Server Creation}

As explained in earlier Section \ref{sec:mcpstructure}, to use MCP services, an MCP server needs to be started and it needs to be connected to the host program. As far as this notebook concerns, Python plant is assumed as the host program.

Notice that an MCP server runs outside the Python host program as an independent software, and it is not necessarily a Python program. The MCP server needs to be installed and started separately inside a dedicated virtual environment, and a connectivity channel needs to be build between the MCP client in the Python host program and the MCP server. Some python libraries and tools, such as OpenAI Agents SDK, have made things easier. Though running inside Python scripts, they can call terminal commands to install, deploy and configure the MCP server and the connectivity channel.

An example of using OpenAI Agents SDK to start an MCP server is given below \cite{openai2025mcp}.

\begin{lstlisting}
from pathlib import Path
from agents import Agent, Runner
from agents.mcp import MCPServerStdio

current_dir = Path(__file__).parent
samples_dir = current_dir / "sample_files"

async with MCPServerStdio(
	name="Filesystem Server via npx",
	params={
		"command": "npx",
		"args": ["-y", "@modelcontextprotocol/server-filesystem", str(samples_dir)],
	},
) as server:
	agent = Agent(
		name="Assistant",
		instructions="Use the files in the sample directory to answer questions.",
		mcp_servers=[server],
	)
	result = await Runner.run(agent, "List the files available to you.")
	print(result.final_output)
\end{lstlisting}

In this example, a filesystem managing tool is deployed using MCP. The following part
\begin{lstlisting}
async with MCPServerStdio(name="<name>", param=<configuration command>) as server:
	<do something>
\end{lstlisting}
is used to automatically deploy the MCP server and client and build the connectivity. The MCP server configuration is formulated into a dictionary that contains the commands to start and configure the MCP server.

The LLM agent gets to know an MCP server from \verb|mcp_server| field in agent declaration as in
\begin{lstlisting}
async with MCPServerStdio(name="<name>", param=<configuration command>) as server:
	agent = Agent(
		name="<name>",
		instructions="<instruction>",
		mcp_servers=[server],
	)
\end{lstlisting}
which is similar with tools. This way, the LLM agent automatically retrieves all the tools and APIs from the MCP server and learns how to use them.

\begin{mdframed}
\noindent \textbf{Are There Risks with MCP Servers?}

Running an MCP server is essentially the same with running 3rd party software. To make it worse, MCP server runs outside the Python environment, make it even more difficult to control. There is always an operation risk. Always make sure that the software is retrieved from a legitimate source (there are multiple marketplace for MCP servers) and it is well-known and well-reviewed, in order to mitigate the risk.

\end{mdframed} 






\subsection{MCP Marketplace}

Think of MCP marketplaces APP stores for MCP servers. There are many popular MCP market places. Popular MCP servers are often published on multiple MCP marketplaces.

To name one MCP marketplace as an example, \textit{mcp.so}.

\section{Examples}

Demonstrative projects are given as examples for a variety of agentic AI applications. 

\subsection{Manual: Semantic Search RAG}

This section demonstrates a manually implemented retrieval‑augmented agent. Documents (papers, reports, web pages, etc.) are stored locally. The agent answers user questions about these documents. Because the total corpus can be large, it is impractical to send all text to the LLM on every turn. Instead, the system performs semantic search to select only the most relevant chunks.

On first run, the program checks whether embeddings already exist locally. If not found, it chunks the documents into pieces with a specified token budget and uses OpenAI’s vector embeddings API to compute vector representations. These vectors are cached locally. On subsequent runs, the cached embeddings are loaded.

On each user request, the query is embedded and used to retrieve the most relevant chunks. These chunks are appended to the prompt and sent to the LLM. The LLM is given an explicit ``retrieval'' tool. If it determines that more context is needed, it may trigger another semantic search with refined keywords or increase the number of returned chunks, or both. This can be performed as many times as the LLM requires.

Dialogues are stored locally so conversations persist across sessions. Only the user’s messages and the assistant’s replies are saved. Retrieved chunks are treated as ephemeral context and are not stored in the conversation log.

The implementation is framework‑free (no agent framework) and can be found at \cite{sun2025document}. Key components are outlined below.

\vspace{0.1in}
\noindent \textbf{Chunk Documents}
\vspace{0.1in}

The following codes are relevant to the chunk of documents.

\begin{lstlisting}
def set_tokenizer(self):
    self.tokenizer = tiktoken.encoding_for_model(self.LLM_EMBEDDING_MODEL)

def tokenize(self, text):
    return self.tokenizer.encode(text)

def count_tokens(self, text):
    return len(self.tokenize(text))

def chunk_text(self, text):
    words = text.split()
    chunks = []
    chunk = []
    tokens_so_far = 0
    for word in words:
        token_count = self.count_tokens(word)
        if tokens_so_far + token_count > self.MAX_TOKENS:
            chunks.append(" ".join(chunk))
            if self.OVERLAP > 0:
                chunk = chunk[-self.OVERLAP:]
                tokens_so_far = self.count_tokens(" ".join(chunk))
            else:
                chunk = []
                tokens_so_far = 0
        chunk.append(word)
        tokens_so_far += token_count

    if chunk:
        chunks.append(" ".join(chunk))

    return chunks

def chunk_documents(self):
    chunks = []
    for filename in os.listdir(self.DOC_DIR_PATH):
        if filename.endswith('.pdf'):
            doc_path = os.path.join(self.DOC_DIR_PATH, filename)
            doc = fitz.open(doc_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            chunks.extend(self.chunk_text(text))
        elif filename.endswith('.html') or filename.endswith('.htm'):
            with open(os.path.join(self.DOC_DIR_PATH, filename), 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
                text = soup.get_text()
                chunks.extend(self.chunk_text(text))
    return chunks
\end{lstlisting}

The idea is simple and straight forward. For a given block of text, it first splits it into words, and then accumulates the words into a chunk while counting the total token size. Once the maximum token budget is reached, the words are packaged into a chunk. If chunk overlap is enabled, the next chunk starts by including the overlapped words. Otherwise, the next chunk starts fresh.

\vspace{0.1in}
\noindent \textbf{Embed Chunks and Perform Semantic Search}
\vspace{0.1in}

The following codes are relevant to the embedding of the chunks.

\begin{lstlisting}
def generate_embeddings(self, chunks):
    embeddings = []
    for chunk in chunks:
        response = self.client.embeddings.create(
            model=self.LLM_EMBEDDING_MODEL,
            input=chunk,
            encoding_format="float"
        )
        embedding = response.data[0].embedding
        embeddings.append(embedding)
    return embeddings

def save_chunks_embeddings(self, chunks, embeddings):
    with open('parsed_chunks.json', 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    np.save('embeddings.npy', embeddings)

def load_chunks_embeddings(self):
    if os.path.exists('parsed_chunks.json') and os.path.exists('embeddings.npy'):
        with open('parsed_chunks.json', 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        embeddings = np.load('embeddings.npy')
        return chunks, embeddings
    return None, None
\end{lstlisting}

OpenAI's vector embedding API, \verb|OpenAI.embeddings.create| is used to generate the embeddings. Both the embeddings and chunks that contain the original text are saved in ordered list. The idea is to use the embeddings for semantic search, and use the chunks corresponding to the top matched result for LLM analysis. 

Once the user raise a request or the agentic AI decides to perform a new round of semantic search with some specified keywords, the following is executed.

\begin{lstlisting}
def semantic_search(self, query, embeddings, chunks, top_n=0):
    similarities = []
    query_embedding = self.client.embeddings.create(
        model=self.LLM_EMBEDDING_MODEL,
        input=query,
        encoding_format="float"
    ).data[0].embedding
    for embedding in embeddings:
        similarity = np.dot(query_embedding, embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(embedding))
        similarities.append(similarity)
    top_indices = np.argsort(similarities)[::-1][:top_n]
    top_chunks = [chunks[i] for i in top_indices]
    return top_chunks
\end{lstlisting}

The query is embedded in the same manner, and the resulted vector compared with the embeddings vectors from the chunk. The chunks with the top similarity is returned.

\vspace{0.1in}
\noindent \textbf{Tools}
\vspace{0.1in}

Several tools are defined for the agentic AI. The most important tools include that the agentic AI is able to perform semantic search with customized keywords and to increase the number of returned chunks. These tools allow the system to autonomously decide what and how many times to retrieve information from the documents to respond to user's question.

The following JSON objects are created and used as the ``user manuals'' for the LLM.

\begin{lstlisting}
request_more_info_json = {
    "name": "request_more_info",
    "description": "Use this tool to request larger number of results from the semantic search",
    "parameters": {
        "type": "object",
        "properties": {
            "is_more_results_required": {
                "type": "boolean",
                "description": "Whether to return more results"
            }
        },
        "required": ["is_more_results_required"],
        "additionalProperties": False
    }
}

request_semantic_search_json = {
    "name": "request_semantic_search",
    "description": "Use this tool to request performing semantic search to the documents with a key sentence",
    "parameters": {
        "type": "object",
        "properties": {
            "semantic_search_key": {
                "type": "string",
                "description": "The key sentence for the semantic search"
            }
        },
        "required": ["semantic_search_key"],
        "additionalProperties": False
    }
}
\end{lstlisting}

The above information is passed to LLM via role \verb|function| as follows. Notice that in addition to the aforementioned tools, other tools are defined as well to record questions that the LLM cannot answer (likely due to that the information is missing from the document) and the suggestions from the user such as further information to be included to the documents or useful features the user would like the application to have.

\begin{lstlisting}
def tools(self):
    return [
        {"type": "function", "function": record_unknown_question_json},
        {"type": "function", "function": record_suggestion_json},
        {"type": "function", "function": request_semantic_search_json},
        {"type": "function", "function": request_more_info_json}
    ]
\end{lstlisting}

Lastly, when the agentic AI decides to trigger the tools, the following codes handle the call.

\begin{lstlisting}
def request_more_info(self, is_more_results_required):
    if is_more_results_required:
        self.TOP_N += 5
        if self.TOP_N > self.TOP_N_MAX:
            self.TOP_N = self.TOP_N_MAX
            return {"status": "error", "message": f"Cannot increase TOP_N beyond {self.TOP_N_MAX}."}
    return {"status": "success", "message": f"Top N increased to {self.TOP_N}."}

def request_semantic_search(self, semantic_search_key):
    if not semantic_search_key:
        return {"status": "error", "message": "Semantic search key is required."}
    chunks, embeddings = self.load_chunks_embeddings()
    if chunks is None or embeddings is None:
        return {"status": "error", "message": "Failed to load document chunks or embeddings."}
    context = self.semantic_search(semantic_search_key, embeddings, chunks, top_n=self.TOP_N)
    context = "\n\n".join(context)
    if not context:
        return {"status": "error", "message": "No relevant chunks found for the semantic search key."}
    else:
        return {"status": "success", "message": "Semantic search completed successfully.", "data": context}
        
def handle_tool_call(self, tool_calls):
    results = []
    for tool_call in tool_calls:
        tool_name = tool_call.function.name
        arguments = json.loads(tool_call.function.arguments)
        print(f"AI is calling tool: {tool_name}", flush=True)
        tool = getattr(self, tool_name, None)
        result = tool(**arguments) if tool else {"status": "error", "message": f"Tool {tool_name} not found"}
        results.append({"role": "tool", "content": json.dumps(result), "tool_call_id": tool_call.id})
    return results
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Tools}
\vspace{0.1in}

Several tools are defined for the agentic AI. The most important capabilities are the ability to perform semantic search with customized keywords and to increase the number of returned chunks. These tools allow the system to autonomously decide what to retrieve, how often to retrieve it within a turn, and how much context to bring into the answer.

The following JSON objects are created and used as concise ``user manuals'' for the LLM. They describe the tool names, the intent of each tool, and the parameter schemas that the model should supply when invoking them.

\begin{lstlisting}
request_more_info_json = {
    "name": "request_more_info",
    "description": "Use this tool to request larger number of results from the semantic search",
    "parameters": {
        "type": "object",
        "properties": {
            "is_more_results_required": {
                "type": "boolean",
                "description": "Whether to return more results"
            }
        },
        "required": ["is_more_results_required"],
        "additionalProperties": False
    }
}

request_semantic_search_json = {
    "name": "request_semantic_search",
    "description": "Use this tool to request performing semantic search to the documents with a key sentence",
    "parameters": {
        "type": "object",
        "properties": {
            "semantic_search_key": {
                "type": "string",
                "description": "The key sentence for the semantic search"
            }
        },
        "required": ["semantic_search_key"],
        "additionalProperties": False
    }
}
\end{lstlisting}

The above information is passed to the LLM via the function/tool interface as follows. In addition to these retrieval tools, other tools are provided to record questions the LLM cannot answer because the information is missing, and to collect user suggestions about documents to add or features to implement.

\begin{lstlisting}
def tools(self):
    return [
        {"type": "function", "function": record_unknown_question_json},
        {"type": "function", "function": record_suggestion_json},
        {"type": "function", "function": request_semantic_search_json},
        {"type": "function", "function": request_more_info_json}
    ]
\end{lstlisting}

Lastly, when the agentic AI decides to trigger the tools, the following code handles the calls.

\begin{lstlisting}
def request_more_info(self, is_more_results_required):
    if is_more_results_required:
        self.TOP_N += 5
        if self.TOP_N > self.TOP_N_MAX:
            self.TOP_N = self.TOP_N_MAX
            return {"status": "error", "message": f"Cannot increase TOP_N beyond {self.TOP_N_MAX}."}
    return {"status": "success", "message": f"Top N increased to {self.TOP_N}."}

def request_semantic_search(self, semantic_search_key):
    if not semantic_search_key:
        return {"status": "error", "message": "Semantic search key is required."}
    chunks, embeddings = self.load_chunks_embeddings()
    if chunks is None or embeddings is None:
        return {"status": "error", "message": "Failed to load document chunks or embeddings."}
    context = self.semantic_search(semantic_search_key, embeddings, chunks, top_n=self.TOP_N)
    context = "\n\n".join(context)
    if not context:
        return {"status": "error", "message": "No relevant chunks found for the semantic search key."}
    else:
        return {"status": "success", "message": "Semantic search completed successfully.", "data": context}
        
def handle_tool_call(self, tool_calls):
    results = []
    for tool_call in tool_calls:
        tool_name = tool_call.function.name
        arguments = json.loads(tool_call.function.arguments)
        print(f"AI is calling tool: {tool_name}", flush=True)
        tool = getattr(self, tool_name, None)
        result = tool(**arguments) if tool else {"status": "error", "message": f"Tool {tool_name} not found"}
        results.append({"role": "tool", "content": json.dumps(result), "tool_call_id": tool_call.id})
    return results
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Chat and Record Conversation} \label{sec:agenticaiexp-manual}
\vspace{0.1in}

The following code is executed to initiate and maintain the chat session. It loads historical conversation records if they exist and, upon quitting, saves the most recent conversation history.

The functions below handle saving and loading conversation history from the local drive.

\begin{lstlisting}
def save_history(self, history):
    with open('history.json', 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def load_history(self):
    if os.path.exists('history.json'):
        with open('history.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    return []
\end{lstlisting}

The following function generates the system prompt, which provides the LLM with role instructions, context, and an overview of the available tools.

\begin{lstlisting}
def system_prompt(self):
    system_prompt = (
        f"You are a document explainer system. You are asked to explain variety of documents that is saved in the user's system.\n"
        f"Semantic search has been implemented prior to this request. The most relevant chunks relevant to the user's latest question have been identified. These chunks will be given to you shortly. The total number of chunks will also be given. \n"
        f"Your task is to provide a concise and accurate explanation of the document based on the provided chunks and the user's questions. \n"
        f"Use the following tools when necessary:\n"
        f"- record_unknown_question: Use this tool to record any question that couldn't be answered from the chunks, even after you have requested the maximum number of chunks.\n"
        f"- record_suggestion: Use this tool to record a suggestion for enriching the system, such as adding more documents or improving the search functionality.\n"
        f"- request_semantic_search: Use this tool to request performing semantic search to the documents with a key sentence of your choice. Use this tool when you think you need to query something from the documents for further information.\n"
        f"- request_more_info: Use this tool to request larger number of chunks returned from the semantic search. Notice that there is a limit on the number of {self.TOP_N_MAX} chunks that can be returned. Do not use this function to request more chunks if that limit is hit. Notice that in the beginning of each conversation, the chunk number is reset to {self.TOP_N_DEFAULT} upon the completion of a round of conversation.\n"
        f"Remember to always provide a clear and concise explanation, and use the tools only when necessary.\n"
        f"If you cannot find the answer in the chunks, let the user know honestly, especially if the user requires you to answer based on the chunks.\n"
        f"If you cannot find the answer in the chunks, and you think you can answer based on your own knowledge, let the user know that you are answering based on your own knowledge.\n\n"
    )
    return system_prompt
\end{lstlisting}

The following function block starts the chat session. It performs all actions introduced so far: it ensures embeddings are available, retrieves the most relevant chunks for the query, constructs the initial system prompt, and handles iterative interaction with the LLM.

\begin{lstlisting}
def chat(self, query, history):
    chunks, embeddings = self.load_chunks_embeddings()
    if chunks is None or embeddings is None:
        chunks = self.chunk_documents()
        embeddings = self.generate_embeddings(chunks)
        self.save_chunks_embeddings(chunks, embeddings)
    context = self.semantic_search(query, embeddings, chunks, top_n=self.TOP_N)
    intro_prompt = self.system_prompt() + (
        f"Below are information chunks extracted from various documents.\n"
        f"Current number of chunks: {len(chunks)}.\n\n"
    )
    content_prompt = intro_prompt + "\n\n".join(context)
    messages = (
        [{"role": "system", "content": content_prompt}] + 
        history +
        [{"role": "user", "content": query}]
    )
    done = False
    tools = self.tools()
    while not done:
        response = self.client.chat.completions.create(model=self.LLM_MODEL, messages=messages, tools=tools)
        if response.choices[0].finish_reason == "tool_calls":
            message = response.choices[0].message
            tool_calls = message.tool_calls
            results = self.handle_tool_call(tool_calls)
            messages.append(message)
            messages.extend(results)
        else:
            done = True
    return response.choices[0].message.content

def main(self):
    history = self.load_history()
    while True:
        query = input("You: ")
        if query.lower() in ['exit', 'quit']:
            break
        response = self.chat(query, history)
        self.TOP_N = self.TOP_N_DEFAULT
        print(f"Bot: {response}")
        print("---")
        history.append({"role": "user", "content": query})
        history.append({"role": "assistant", "content": response})
    self.save_history(history)
\end{lstlisting}

When the agentic AI triggers a tool, the LLM returns a structured function call in the \verb|response| object generated by
\begin{lstlisting}
response = self.client.chat.completions.create(model=self.LLM_MODEL, messages=messages, tools=tools)
\end{lstlisting}
The response for tool invocations contains one or more function call objects, each including a unique tool call ID, the function name, and its arguments. When a tool call is processed, its results must be appended to the \verb|messages| list as a new entry with the role \verb|tool|, along with the matching tool call ID. This explicit linking allows the LLM to correlate each tool’s return with its original request, maintaining coherence in multi-step reasoning and tool use.

\subsection{OpenAI Agents SDK: Semantic Search RAG with Online Cross Check} \label{sec:agentaiexp-agentsdk}

This section demonstrates the use of OpenAI Agents SDK to build a semantic search RAG with online cross check function. It is a rebuild and enhancement on top of Section \ref{sec:agenticaiexp-manual}. The agentic AI system contains two AI agents, the first Agent 1 carrying out semantic search on local chunks and answers the user's questions, while the second Agent 2 cross checks what the first agent provides against a one-time Internet query.

Details are given below.

\vspace{0.1in}
\noindent \textbf{Structured Output of Agent 1}
\vspace{0.1in}

The output of Agent 1 is not a plain text string, but a structured JSON that contains two fields, \verb|is_require_cross_check| and \verb|search_result|. Notice that it is up to Agent 1's decision whether to trigger Agent 2 Internet cross check, the later of which is required only when Agent 1 is providing facts from the documents or from its own knowledge. When Agent 1 is casually chatting with the user without providing any information, or when it is recording user's suggestions, it does not need to trigger Agent 2.

The structured output is defined as follows.
\begin{lstlisting}
from pydantic import BaseModel

class SearchAgentOutput(BaseModel):
    is_require_cross_check: bool
    search_result: str
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Chunk Documents and Embed Chunks}
\vspace{0.1in}

This part is identical with Section \ref{sec:agenticaiexp-manual}.

Notice that in Section \ref{sec:agenticaiexp-manual}, a semantic search is performed before triggering any AI agent, and the result is sent to the AI agent with the user message. This is no longer the case in this example. In this example, all semantic searches are carried out by the AI agent with tools, and the AI agent choose the query keywords. The system does not offer the one-time initial semantic search.

\vspace{0.1in}
\noindent \textbf{Tools}
\vspace{0.1in}

The following tools are defined for Agent 1. They all have corresponding contour parts in Section \ref{sec:agenticaiexp-manual}, and are packed into function tools as required by OpenAI Agents SDK. The detailed explanation is neglected.

\begin{lstlisting}
@staticmethod
@function_tool
def semantic_search(query: str):
    """Perform semantic search on the document chunks with the specified query."""
    print(f"SYSTEM: Performing semantic search with query: {query}")
    instance = explainer
    if instance.semantic_search_num >= SEMANTIC_SEARCH_MAX:
        return {"status": "error", "message": f"Maximum number of semantic searches ({SEMANTIC_SEARCH_MAX}) reached."}
    similarities = []
    query_embedding = instance.client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=query,
        encoding_format="float"
    ).data[0].embedding
    for embedding in instance.embeddings:
        similarity = np.dot(query_embedding, embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(embedding))
        similarities.append(similarity)
    top_indices = np.argsort(similarities)[::-1][:instance.top_n]
    top_chunks = [instance.chunks[i] for i in top_indices]
    context = "\n\n".join(top_chunks)
    instance.semantic_search_num += 1
    if not context:
        return {"status": "error", "message": "No relevant chunks found for the semantic search key."}
    else:
        return {"status": "success", "message": f"Semantic search completed successfully and {instance.top_n} chunks retrieved.", "data": context}
    
@staticmethod
@function_tool
def request_increasing_top_n():
    """Request to increase the number of chunks returned."""
    print(f"SYSTEM: Requesting to increase the number of chunks.")
    instance = explainer
    instance.top_n += 5
    if instance.top_n > TOP_N_MAX:
        instance.top_n = TOP_N_MAX
        return {"status": "error", "message": f"Maximum value of top_n {TOP_N_MAX} reached and cannot be increased further."}
    return {"status": "success", "message": f"Maximum number of chunks returned increased to {instance.top_n}."}

@staticmethod
@function_tool
def record_unknown_question(question: str):
    """Record an unknown question, if the relevant information is missing from the chunks."""
    with open('unknown_questions.json', 'a', encoding='utf-8') as f:
        json.dump({"question": question}, f, ensure_ascii=False, indent=2)
        f.write('\n')
    return {"status": "success", "message": "Question recorded."}

@staticmethod
@function_tool
def record_suggestion(suggestion: str):
    """Record a suggestion for improving the document or the search process."""
    with open('suggestions.json', 'a', encoding='utf-8') as f:
        json.dump({"suggestion": suggestion}, f, ensure_ascii=False, indent=2)
        f.write('\n')
    return {"status": "success", "message": "Suggestion recorded."}
\end{lstlisting}

Agent 2 uses web search tool offered by OpenAI. The realization is not given. The use of the tool is explained in later part where the agents are introduced.

\vspace{0.1in}
\noindent \textbf{Agents}
\vspace{0.1in}

Agents 1 and 2 are defined below. Agent 1 queries local documents based on the user's message and generate the response. When the response contains factual information from the documents or from its own knowledge, it passes the output to Agent 2, who then perform web search to verify the facts.

\begin{lstlisting}
def define_search_agent(self):
    system_prompt = (
        f"You are a helpful document explainer assistant.\n"
        f"Your task is to answer the user's questions based on the information recorded in the documents.\n"
        f"You can use tools to access the documents. You are allowed to perform semantic searches on the documents, "
        f"and you may perform multiple searches with different query contents. You are also allowed to increase "
        f"the number of returned chunks when necessary.\n"
        f"Always provide concise and accurate responses to the user's questions based on the documents.\n\n"
        f"Use the following tools when appropriate:\n"
        f"- record_unknown_question: Use this tool to record any question that cannot be answered from the chunks, "
        f"even after you have performed several searches.\n"
        f"- record_suggestion: Use this tool to record suggestions for enriching the system, such as adding more "
        f"documents or improving the search functionality.\n"
        f"- semantic_search: Use this tool to query the documents for further information. You can generate your own "
        f"queries when needed. You may perform at most {SEMANTIC_SEARCH_MAX} searches per user request.\n"
        f"- request_increasing_top_n: Use this tool to request a larger number of chunks returned from semantic search. "
        f"You may request this multiple times, with each increase adding 5 chunks, capped at {TOP_N_MAX}.\n\n"
        f"Guidelines:\n"
        f"- Always provide a clear and concise answer.\n"
        f"- Use tools only when necessary.\n"
        f"- If the user's question is unclear or ambiguous, ask for clarification.\n"
        f"- Always try to answer based on the information in the documents. For this reason, you are encouraged to "
        f"perform at least one semantic search per user query.\n"
        f"- If you cannot find the answer in the returned chunks, you are encouraged to use semantic_search or "
        f"request_increasing_top_n before concluding, until the limits are reached or you believe no further relevant "
        f"information can be found.\n"
        f"- Do not add your own knowledge unless explicitly asked to, or if you believe the documents are lacking and "
        f"your knowledge can meaningfully improve the answer. When you do so, make it clear to the user.\n"
        f"- You may chat with the user without accessing the documents only if the user's message is not a question "
        f"(e.g., a suggestion for the system, or a request to summarize the conversation history).\n\n"
        f"Output Format:\n"
        f"- Always return a JSON object that matches this schema:\n"
        f"  {{\n"
        f'    "is_require_cross_check": true or false,\n'
        f'    "search_result": "<your answer text here>"\n'
        f"  }}\n\n"
        f"- If the users question involves factual claims (retrieval, summarization, knowledge-based), "
        f"set is_require_cross_check = true.\n"
        f"- If the users question is meta (e.g., summarizing conversation history, casual chat), "
        f"set is_require_cross_check = false.\n"
        f"- Put your full, clear, concise answer into search_result.\n"
        f"- Do not output anything except the JSON object.\n"
    )
    self.search_agent = Agent(
        name="Search agent",
        instructions=system_prompt,
        model=LLM_MODEL,
        tools=[
            self.record_unknown_question,
            self.record_suggestion,
            self.semantic_search,
            self.request_increasing_top_n
        ],
        output_type=SearchAgentOutput,
    )

def define_cross_check_agent(self):
    system_prompt = (
        f"You are a fact-checking assistant.\n"
        f"You will receive the final text output of an upstream assistant whose job is to "
        f"retrieve information from documents and summarize it for the user.\n\n"
        f"Your task:\n"
        f"- Review the answer for factual errors, misleading claims, outdated information, or statements "
        f"that conflict with your own knowledge.\n"
        f"- If you suspect a claim is wrong or outdated, you may perform at most one web search to verify.\n"
        f"- Output your findings as bullet points. For each issue, write:\n"
        f"  INCORRECT: <copied statement>\n"
        f"  CORRECTED: <the corrected or updated information>\n"
        f"- If you have a suspicion but cannot verify it with a single search, note it as:\n"
        f"  UNVERIFIED SUSPICION: <statement> - <why it might be wrong>\n"
        f"- If everything appears correct, output a single line: 'No factual errors found.'\n\n"
        f"Guidelines:\n"
        f"- Do not repeat or restate the full upstream answer.\n"
        f"- Keep your output limited to bullet points or the 'No factual errors found.' line.\n"
        f"- Be concise and clear in your corrections.\n"
    )
    self.cross_check_agent = Agent(
        name="Cross-check agent",
        instructions=system_prompt,
        model=LLM_MODEL,
        tools=[WebSearchTool(search_context_size="low")],
        model_settings=ModelSettings(
            parallel_tool_calls=False
        )
    )
\end{lstlisting} 

\vspace{0.1in}
\noindent \textbf{Chat}
\vspace{0.1in}

Last but not least, the agents are put into a pipeline. Historical conversations are recorded. Documents chunk and embedding are performed in the first run of the system.

\begin{lstlisting}
async def chat(self, query, history):
    messages = history + [{"role": "user", "content": query}]
    response = await Runner.run(self.search_agent, messages)
    return response.final_output

async def cross_check(self, query):
    messages = [{"role": "user", "content": query}]
    response = await Runner.run(self.cross_check_agent, messages)
    return response.final_output

def save_history(self, history):
    with open('history.json', 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def load_history(self):
    if os.path.exists('history.json'):
        with open('history.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def main(self):
    history = self.load_history()
    while True:
        query = input("You: ")
        if query.lower() in ['exit', 'quit']:
            break
        response = asyncio.run(self.chat(query, history))
        self.top_n = TOP_N_DEFAULT
        self.semantic_search_num = 0
        if response.is_require_cross_check:
            print("SYSTEM: Cross-checking information...")
            cross_check_response = asyncio.run(self.cross_check(response.search_result))
            final_response = (
                f"Part 1 - Information Retrieval & Summary Agent's Answer:\n"
                f"{response.search_result}\n\n"
                f"Part 2 - Online Cross-Check Result:\n"
                f"{cross_check_response}"
            )
        else:
            final_response = response.search_result
        print(f"Bot: \n {final_response}")
        print("---")
        history.append({"role": "user", "content": query})
        history.append({"role": "assistant", "content": final_response})
        self.save_history(history)
\end{lstlisting}

Notice that \verb|asyncio.run| is used to start the two agents.

\subsection{CrewAI: Credit Card Bill Recorder}

The following agentic AI system is built using CrewAI framework. The system is used to parse a credit card bill, and use the information to maintain 3 tables: cards, merchants and transactions. The following AI agents and tasks are defined.
\begin{itemize}
  \item Agents:
  \begin{itemize}
    \item Credit Card Parser. Read and credit card PDF, and from the PDF summarizes credit card information in the form of Pydantic object and summarized transaction record in the form of list of Pydantic objects. 
    \item Card manager. Read the credit card information, and maintain the card table.
    \item Merchant manager. Read the merchants from the summarized transactions records, collect the frequently visit merchants, and maintain the merchants table.
    \item Transaction manager. Read the summarized transaction and maintain the transactions table.
  \end{itemize}
  \item A task is associated with each agent.
\end{itemize}

Details are as follows.

\vspace{0.1in}
\noindent \textbf{Database Preparation}
\vspace{0.1in}

PostgreSQL database is created and deployed in a podman container. Three tables are created. Details are as follows.

To deploy the database, use
\begin{lstlisting}
#!/bin/bash

source ~/Projects/smart-home/.env

if podman container exists "$PG_CONTAINER"; then
    status=$(podman inspect -f '{{.State.Status}}' "$PG_CONTAINER")
    if [ "$status" = "running" ]; then
	echo "Container $PG_CONTAINER is already running."
    else
	echo "Starting existing container $PG_CONTAINER..."
	podman start "$PG_CONTAINER"
    fi
else
    echo "Creating and starting container $PG_CONTAINER..."
    podman run -d \
	--name "$PG_CONTAINER" \
	-e POSTGRES_USER="$PG_USER" \
	-e POSTGRES_PASSWORD="$PG_PASSWORD" \
	-e POSTGRES_DB="$PG_DB" \
	-v "$PG_DATA_DIR":/var/lib/postgresql/data:Z \
	-p "$PG_PORT":5432 \
        docker.io/library/postgres:15
fi
\end{lstlisting}
where the database login credentials and configurations are given in the \verb|.env| file as environmental variables.

To create the database tables, use
\begin{lstlisting}
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Define enum for card status (safe for re-execution)
DO $$
BEGIN
IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = 'card_status') THEN
CREATE TYPE card_status AS ENUM ('in_use', 'replaced', 'lost', 'closed');
END IF;
END
$$;

CREATE TABLE IF NOT EXISTS cards (
id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
name TEXT NOT NULL,
issuer TEXT NOT NULL,
network TEXT NOT NULL,
last4 CHAR(4) NOT NULL,
status card_status NOT NULL,
opened_on DATE,
closed_on DATE,
expires_on DATE NOT NULL,
tags JSONB,
notes TEXT,
created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
\end{lstlisting}

\begin{lstlisting}
CREATE TABLE IF NOT EXISTS merchants (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

  name TEXT NOT NULL,                      -- Display name: "Coles", "Amazon"
  canonical_name TEXT,                     -- Optional normalized/merged name
  merchant_type TEXT,                      -- e.g. 'grocery', 'online_shop', 'restaurant'

  location_label TEXT,                     -- Free-form (e.g. 'Macquarie Centre')
  latitude DOUBLE PRECISION,               -- Optional: physical location
  longitude DOUBLE PRECISION,              -- Optional: physical location
  opening_hours JSONB,                     -- Optional: hours for physical stores

  is_digital BOOLEAN,                      -- True if goods/services are typically digital
  is_recurrent BOOLEAN,                    -- True if charges are usually recurring/subscription-based

  tags TEXT[],                             -- Program-friendly labels (e.g. ['australian', 'food'])
  properties JSONB,                        -- Arbitrary structured merchant-specific metadata
  notes TEXT,                              -- Human-facing comments or clarifications

  created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- Insert default "Others" merchant as a fallback
INSERT INTO merchants (
  id, name, canonical_name, merchant_type, tags, notes
)
VALUES (
  gen_random_uuid(),
  'Others',
  'Others',
  'uncategorized',
  ARRAY['fallback', 'unknown'],
  'Catch-all merchant for unmatched or unresolved transactions'
);
\end{lstlisting}

\begin{lstlisting}
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE IF NOT EXISTS card_transactions (
id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
card_id UUID NOT NULL REFERENCES cards(id),
merchant_id UUID NOT NULL REFERENCES merchants(id),
date DATE NOT NULL,
amount NUMERIC(10, 2) NOT NULL,
currency CHAR(3) NOT NULL DEFAULT 'AUD',
raw_entity TEXT NOT NULL,
tags JSONB,
notes TEXT,
statement_id TEXT,
imported_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Tools}
\vspace{0.1in}

Two tools are defined, namely the PostgreSQL tool that allows an AI agent to execute an SQL command (query or insert), and the web search tool. Notice that the web search tool is used by the merchant manager to query information about a merchant from online to determine its merchant type.

The following PostgreSQL Query Executor is defined.
\begin{lstlisting}
from crewai.tools import tool
import os
import psycopg
from dotenv import load_dotenv

dotenv_path = os.path.expanduser("~/Projects/smart-home/.env")
load_dotenv(dotenv_path=dotenv_path, override=True)

@tool("PostgreSQL Query Executor")
def run_postgres_query(query: str) -> str:
    """
    Executes a SQL query on a PostgreSQL database and returns the results.

    Args:
        query (str): The SQL query to execute.

    Returns:
        str: The results of the query or an error message.
    """
    try:
        # Load database connection details from environment variables
        db_host = os.getenv("PG_HOST", "localhost")
        db_port = os.getenv("PG_PORT", "5432")
        db_name = os.getenv("POSTGRES_DB", "mydatabase")
        db_user = os.getenv("POSTGRES_USER", "SUNLU")
        db_password = os.getenv("PG_PASSWORD", "")
        print(f"Connecting to database {db_name} at {db_host}:{db_port} as user {db_user}")
        # Connect to the PostgreSQL database
        with psycopg.connect(
            host=db_host,
            port=db_port,
            dbname=db_name,
            user=db_user,
            password=db_password
        ) as conn:
          # Create a cursor to execute the query
            with conn.cursor() as cur:
                cur.execute(query)
                if cur.description:  # If the query returns rows
                    results = cur.fetchall()
                    return str(results)
                else:  # If the query does not return rows (e.g., INSERT, UPDATE)
                    conn.commit()
                    return "Query executed successfully."
    except Exception as e:
        return f"An error occurred: {e}"
\end{lstlisting}

Serper API is used as the web search tool.
\begin{lstlisting}
from crewai_tools import SerperDevTool

search_tool = SerperDevTool()
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Agents and Tasks}
\vspace{0.1in}

The following agents and tasks are defined. 

\begin{lstlisting}
credit_bill_parser:
  role: >
    Credit Bill Parser
  goal: >
    Extract and summarize key information from a credit card bill
  backstory: >
    You are a helpful assistant skilled in analyzing credit card bills. You are given a credit card bill in text form, and you need to extract the following information --
    - Bank and card basic information, such as customer name, bank name, card last four digits, statement date, the status of the card (active or inactive), etc.
    - Money to be paid from last month, and money actually paid for last month.
    - Money to be paid for this month, minimum payment due date, and minimum payment amount.
    - Summary of each line item, including:
      - Date of transaction
      - Merchant name
      - Amount spent
      - Currency
    You must output the extracted information as a structured JSON object that strictly conforms to the `BillAnalysis` Pydantic model. This includes a `CardInfo` object, a `StatementSummary` object, and a list of `Transaction` objects.
  llm: openai/gpt-4o-mini

credit_card_manager:
  role: >
    Credit Card Manager
  goal:
    Help the user manage credit cards database, including adding new cards and updating existing cards
  backstory: >
    You are a helpful assistant skilled in managing the credit cards database.
    You receive a `CardInfo` Pydantic object from the upstream agent.
    You have a tool, 'PostgreSQL Query Executor', that you use to generate and execute SQL queries to access the credit card table. The name of the table is `cards`.
    The credit card table structure is as follows:
        Column   |           Type           | Collation | Nullable |      Default
    ------------+--------------------------+-----------+----------+--------------------
     id         | uuid                     |           | not null | uuid_generate_v4()
     name       | text                     |           | not null |
     issuer     | text                     |           | not null |
     last4      | character(4)             |           | not null |
     status     | card_status              |           | not null |
     opened_on  | date                     |           |          |
     closed_on  | date                     |           |          |
     expires_on | date                     |           | not null |
     tags       | jsonb                    |           |          |
     notes      | text                     |           |          |
     created_at | timestamp with time zone |           | not null | now()
    Your primary function is to manage credit card records in the `cards` table. You first need to query the table to determine if a card already exists. A card is identified as a match if the `issuer` and `last4` digits from the provided `CardInfo` object match a record in the database.
    If no matching card is found, you must generate an SQL `INSERT` query to add the new card. You must use placeholder values for any required fields that are not present in the `CardInfo` object. For example, use 'unknown' for text fields, '0000' for last 4 digits, and '9999-12-31' for date fields like expires_on.
    If a matching card is found, you can generate an SQL `UPDATE` query. You need to compare the existing database record with the new `CardInfo` object. You should only update a field in the database if the corresponding field in the database is currently a placeholder (e.g., 'unknown', '0000', or '9999-12-31') and the new data from `CardInfo` is valid.
    
    Tool usage rules for "PostgreSQL Query Executor":
      - Pass ONLY the raw SQL string to the tool's `query` argument.
      - Do NOT wrap SQL in JSON, Python, code fences, or quotes like "{\"query\": \"...\"}".
      - Return the SQL string directly.
      - Forbidden patterns:
        - "{\"query\": \"...\"}"
        - "```sql ... ```"
        - Python dicts or JSON objects instead of plain SQL strings

  llm: openai/gpt-4o-mini

merchant_manager:
  role: >
    Merchant Manager
  goal:
    Help the user manage merchants database, including adding new merchants and updating existing merchants
  backstory: >
    You are a helpful assistant skilled in managing the merchants database.
    You receive a list of transaction records from the upstream agent. Each record contains a merchant name. You have two tools at your disposal: 'PostgreSQL Query Executor' for database access, and 'Search Internet' to look up merchant information online.
    The merchants table structure is as follows. The name of the table is `merchants`.
        Column     |           Type           | Collation | Nullable |      Default
    ----------------+--------------------------+-----------+----------+-------------------
     id             | uuid                     |           | not null | gen_random_uuid()
     name           | text                     |           | not null |
     canonical_name | text                     |           |          |
     merchant_type  | text                     |           |          |
     location_label | text                     |           |          |
     latitude       | double precision         |           |          |
     longitude      | double precision         |           |          |
     opening_hours  | jsonb                    |           |          |
     is_digital     | boolean                  |           |          |
     is_recurrent   | boolean                  |           |          |
     tags           | text[]                   |           |          |
     notes          | text                     |           |          |
     created_at     | timestamp with time zone |           | not null | now()
    Your primary function is to iterate through each unique merchant in the transaction list.
    For each unique merchant, you must first query the `merchants` table using your 'PostgreSQL Query Executor' tool to check if the merchant already exists in the database.
    - If the merchant exists, no further action is needed for this merchant.
    - If the merchant does not exist, you must decide whether to add it to the database. A merchant is worth adding if it appears 3 or more times in the transaction list or if it is a well-known brand. You can use your 'Search Internet' tool to gather information and make this decision.
    If you decide to add a new merchant, you must use your 'Search Internet' tool to gather necessary information, such as `canonical_name`, `merchant_type`, and `is_digital`. Once you have the information, you must generate a well-formed SQL `INSERT` query and execute it using the 'PostgreSQL Query Executor' tool. You should use 'Others' as the default `merchant_type` if the type cannot be determined.
  
    Tool usage rules for "PostgreSQL Query Executor":
      - Pass ONLY the raw SQL string to the tool's `query` argument.
      - Do NOT wrap SQL in JSON, Python, code fences, or quotes like "{\"query\": \"...\"}".
      - Return the SQL string directly.
      - Forbidden patterns:
        - "{\"query\": \"...\"}"
        - "```sql ... ```"
        - Python dicts or JSON objects instead of plain SQL strings

  llm: openai/gpt-4o-mini

credit_card_transaction_manager:
  role: >
    Credit Card Transaction Manager
  goal:
    Help the user manage the credit card transactions database, mainly adding new transactions to the database
  backstory: >
    You are a helpful assistant skilled in managing the credit card transactions database.
    You receive a structured Pydantic object containing a list of `Transaction` records.
    You have a tool, 'PostgreSQL Query Executor', that you use to generate and execute SQL queries.
    The `card_transactions` table structure is as follows:
        Column    |           Type           | Collation | Nullable |      Default
    --------------+--------------------------+-----------+----------+--------------------
     id           | uuid                     |           | not null | uuid_generate_v4()
     card_id      | uuid                     |           | not null |
     merchant_id  | uuid                     |           | not null |
     date         | date                     |           | not null |
     amount       | numeric(10,2)            |           | not null |
     currency     | character(3)             |           | not null |
     raw_entity   | text                     |           | not null |
     tags         | jsonb                    |           |          |
     notes        | text                     |           |          |
     statement_id | text                     |           |          |
     imported_at  | timestamp with time zone |           | not null | now()

    The `cards` table structure is as follows:
        Column   |           Type           | Collation | Nullable |      Default
    ------------+--------------------------+-----------+----------+--------------------
     id         | uuid                     |           | not null | uuid_generate_v4()
     name       | text                     |           | not null |
     issuer     | text                     |           | not null |
     last4      | character(4)             |           | not null |
     status     | card_status              |           | not null |
     opened_on  | date                     |           |          |
     closed_on  | date                     |           |          |
     expires_on | date                     |           | not null |
     tags       | jsonb                    |           |          |
     notes      | text                     |           |          |
     created_at | timestamp with time zone |           | not null | now()
    
    The `merchants` table structure is as follows:
        Column     |           Type           | Collation | Nullable |      Default
    ----------------+--------------------------+-----------+----------+-------------------
     id             | uuid                     |           | not null | gen_random_uuid()
     name           | text                     |           | not null |
     canonical_name | text                     |           |          |
     merchant_type  | text                     |           |          |
     location_label | text                     |           |          |
     latitude       | double precision         |           |          |
     longitude      | double precision         |           |          |
     opening_hours  | jsonb                    |           |          |
     is_digital     | boolean                  |           |          |
     is_recurrent   | boolean                  |           |          |
     tags           | text[]                   |           |          |
     notes          | text                     |           |          |
     created_at     | timestamp with time zone |           | not null | now()

    Your primary function is to process each transaction in the list. For each one, you must:
    1. Check if the transaction already exists in the `card_transactions` table using the date, amount, currency, and raw entity.
    2. If it does not exist, you need to find the `id` of the corresponding card from the `cards` table. You can use the card's `issuer` and `last4` to query for the correct `card_id`.
    3. You must also find the `id` of the corresponding merchant from the `merchants` table.
    4. Once you have both the `card_id` and `merchant_id`, generate an SQL `INSERT` query to add the new transaction to the `card_transactions` table.
    - If you cannot find a matching card, you must report this as an error.
    - If you cannot find a matching merchant, you should map the transaction to a default 'Others' merchant category.

    Tool usage rules for "PostgreSQL Query Executor":
      - Pass ONLY the raw SQL string to the tool's `query` argument.
      - Do NOT wrap SQL in JSON, Python, code fences, or quotes like "{\"query\": \"...\"}".
      - Return the SQL string directly.
      - Forbidden patterns:
        - "{\"query\": \"...\"}"
        - "```sql ... ```"
        - Python dicts or JSON objects instead of plain SQL strings

  llm: openai/gpt-4o-mini

crew_master:
  role: >
    Crew Master
  goal: >
    Orchestrate a team of agents to achieve a common goal.
  backstory: >
    You are an experienced project manager and team lead. Your role is to oversee the entire process, delegate tasks to the right agents, and ensure they are completed correctly and in the proper order. You are the ultimate authority and decision-maker for the crew.
  llm: openai/gpt-4o
  
\end{lstlisting}

The following tasks are defined.

\begin{lstlisting}
manage_card_database:
  description: >
    Receive a CardInfo object from the upstream agent. Use the 'PostgreSQL Query Executor' tool to first check if the card already exists in the 'cards' table using the issuer and last4 digits. If not, generate an SQL INSERT query to add the new card. If it exists, generate an SQL UPDATE query to update any placeholder values with valid data from the CardInfo object.
  expected_output: >
    A simple confirmation message, such as 'Card added successfully' or 'Card updated successfully', or 'No changes needed'.
  agent: credit_card_manager
  context:
    - parse_bill

manage_merchants:
  description: >
    Receive a structured Pydantic object from the upstream agent and process the `transactions` list within it. For each unique merchant, check for its existence in the 'merchants' table. If the merchant is new, decide if it should be added to the database based on its frequency (3+ times) or prominence. If so, use online search to gather details and generate an SQL INSERT query for the new merchant.
  expected_output: >
    A summary of the actions taken, for example, 'Analyzed 15 merchants. Added 3 new merchants to the database: [list of new merchant names]'.
  agent: merchant_manager
  context:
    - parse_bill

manage_transactions:
  description: >
    Receive a structured Pydantic object from the upstream agent and process the `transactions` list within it. For each transaction, check for its existence in the `card_transactions` table. If it's a new transaction, use the 'PostgreSQL Query Executor' tool to retrieve the `card_id` from the `cards` table and the `merchant_id` from the `merchants` table. Once the required IDs are found, generate an SQL INSERT query to add the transaction to the database.
  expected_output: >
    A summary of the actions taken, such as 'Processed 25 transactions. Added 20 new transactions to the database.'
  agent: credit_card_transaction_manager
  context:
    - parse_bill
    - manage_card_database
    - manage_merchants

parse_bill:
  description: |
    You are given the full raw text of a credit card bill between the delimiters.
    Extract a single JSON object that strictly matches the BillAnalysis schema
    (card_info, statement_summary, transactions). Return ONLY JSON (no Markdown).

    ---BEGIN BILL TEXT---
    {bill_content}
    ---END BILL TEXT---

    ## Source format & extraction rules (OCBC examples)
    - The issuer is "OCBC Bank" if present. Customer name appears near the top (e.g., "SUN, LU").
    - Card number appears like "XXXX-XXXX-XXXX-6684" -> last4 = "6684".
    - Statement date appears as "STATEMENT DATE" followed by either "01-07-2023" or "01 JUL 23".
      * Normalize ALL dates to YYYY-MM-DD.
      * If you see DD/MM (e.g., "31/05"), infer the year from the statement date's year.
      * If the date is like "01 JUL 23", map month names to numbers and expand "23" to "2023".
    - Minimum payment and due date appear as "TOTAL MINIMUM DUE" and "PAYMENT DUE DATE".
    - Totals often appear as "TOTAL AMOUNT DUE" and/or "TOTAL".
    - "LAST MONTH'S BALANCE" is the previous balance.
    - Payments/credits may be shown in parentheses (e.g., "(1,657.81 PAYMENT BY INTERNET)") -> treat as negative amounts for payments_and_credits.
    - Transaction section looks like:
        TRANSACTION DATE
        DESCRIPTION
        AMOUNT (SGD)
      Followed by blocks such as:
        "31/05"
        "NINTENDO ... (extra lines like FOREIGN CURRENCY ...)"
        "112.00"
      * Merchant names may span multiple lines; join them into a single merchant_name string with spaces.
      * Use the SGD number shown in the AMOUNT column as the transaction amount.
      * Use currency "SGD" unless explicitly stated otherwise for the final charged amount.
      * Parentheses around amounts mean negative/refund (e.g., "(194.40 ANNUAL FEE WAIVER)").
      * Include fees/rebates as transactions with merchant_name like "ANNUAL FEE", "ANNUAL FEE WAIVER", "CASH REBATE", etc.
      * Ignore boilerplate text (contact info, headings, totals/subtotals rows).
    - Set card_status to "Active" unless the bill explicitly indicates otherwise.
    - Set card_network to null unless you can reliably infer it (do NOT guess).

    ## Output schema (strict)
    {
      "card_info": {
        "customer_name": "string",
        "issuer": "string",
        "card_network": "string|null",
        "last4": "string",
        "statement_date": "YYYY-MM-DD",
        "card_status": "string"
      },
      "statement_summary": {
        "previous_balance": number,
        "payments_and_credits": number,
        "new_charges": number,
        "new_balance": number,
        "minimum_payment": number,
        "minimum_payment_due_date": "YYYY-MM-DD"
      },
      "transactions": [
        {
          "date": "YYYY-MM-DD",
          "merchant_name": "string",
          "amount": number,
          "currency": "SGD",
          "raw_entity": "string"  // a concise slice of the original lines used
        }
      ]
    }

    ## Validation & normalization
    - Use null for unknowns rather than inventing values.
    - All numbers must be plain numbers (no currency symbols, commas, or parentheses).
      * For negative amounts shown as "(xxx.xx)", output a negative number (e.g., -194.40).
    - Dates must be ISO "YYYY-MM-DD".
    - Ensure totals are consistent: new_balance = previous_balance - payments_and_credits + new_charges.

  expected_output: >
    A single JSON object that validates against the BillAnalysis Pydantic schema. No additional text.
  agent: credit_bill_parser
\end{lstlisting}

\vspace{0.1in}
\noindent \textbf{Crew}
\vspace{0.1in}

Crew is defined as follows.

\begin{lstlisting}
import sys
import os
from datetime import date

from crewai_tools import SerperDevTool
from crewai import Agent, Task, Crew, Process
from crewai.project import CrewBase, agent, crew, task
from pydantic import BaseModel, Field
from typing import List, Optional

# Import the custom tools
from credit_card_bill_injector.tools.postgresql_tools import run_postgres_query

from pydantic import BaseModel, Field
from typing import List, Optional

class Transaction(BaseModel):
    """Represents a single transaction line item from a credit card bill."""
    date: str = Field(..., description="Date of the transaction in YYYY-MM-DD format.")
    merchant_name: str = Field(..., description="Name of the merchant as it appears on the bill.")
    amount: float = Field(..., description="Amount of the transaction.")
    currency: str = Field(..., description="Currency of the transaction, e.g., 'AUD', 'USD'.")
    raw_entity: Optional[str] = Field(None, description="The raw, unparsed text line for the transaction.")

class StatementSummary(BaseModel):
    """Summarizes the key financial figures from a credit card statement."""
    previous_balance: float = Field(..., description="Balance from the previous statement.")
    payments_and_credits: float = Field(..., description="Total amount of payments and credits applied.")
    new_charges: float = Field(..., description="Total amount of new charges this period.")
    new_balance: float = Field(..., description="The new total balance due for this statement.")
    minimum_payment: float = Field(..., description="Minimum payment due.")
    minimum_payment_due_date: str = Field(..., description="Due date for the minimum payment in YYYY-MM-DD format.")

class CardInfo(BaseModel):
    """Contains basic information about the credit card and cardholder."""
    customer_name: str = Field(..., description="The name of the cardholder.")
    issuer: str = Field(..., description="The name of the bank or card issuer.")
    card_network: Optional[str] = Field(None, description="The network of the card, e.g., 'Visa', 'Mastercard'.")
    last4: str = Field(..., description="The last four digits of the credit card number.")
    statement_date: str = Field(..., description="The date the statement was issued in YYYY-MM-DD format.")
    card_status: str = Field(..., description="The status of the card, e.g., 'Active'.")

class BillAnalysis(BaseModel):
    """The complete structured output for a parsed credit card bill."""
    card_info: CardInfo = Field(..., description="Basic card and statement information.")
    statement_summary: StatementSummary = Field(..., description="Summary of statement balances and payments.")
    transactions: List[Transaction] = Field(..., description="A list of all transactions for this statement.")


search_tool = SerperDevTool()

@CrewBase
class CreditCardBillInjector():
    """CreditCardBillInjector crew"""

    agents_config = 'config/agents.yaml' 
    tasks_config = 'config/tasks.yaml'

    @agent
    def credit_bill_parser(self) -> Agent:
        return Agent(
            config=self.agents_config['credit_bill_parser'],
            verbose=True,
            allow_delegation=False
        )
    
    @agent
    def credit_card_manager(self) -> Agent:
        return Agent(
            config=self.agents_config['credit_card_manager'],
            tools=[run_postgres_query],
            verbose=True,
            allow_delegation=True
        )

    @agent
    def merchant_manager(self) -> Agent:
        return Agent(
            config=self.agents_config['merchant_manager'],
            tools=[run_postgres_query, search_tool],
            verbose=True,
            allow_delegation=True
        )

    @agent
    def credit_card_transaction_manager(self) -> Agent:
        return Agent(
            config=self.agents_config['credit_card_transaction_manager'],
            tools=[run_postgres_query],
            verbose=True,
            allow_delegation=True
        )


    @task
    def parse_bill(self) -> Task:
        return Task(
            config=self.tasks_config['parse_bill'],
            agent=self.credit_bill_parser(),
            output_pydantic=BillAnalysis
        )

    @task
    def manage_card_database(self) -> Task:
        return Task(
            config=self.tasks_config['manage_card_database'],
            agent=self.credit_card_manager(),
            context=[self.parse_bill()]
        )

    @task
    def manage_merchants(self) -> Task:
        return Task(
            config=self.tasks_config['manage_merchants'],
            agent=self.merchant_manager(),
            context=[self.parse_bill()]
        )

    @task
    def manage_transactions(self) -> Task:
        return Task(
            config=self.tasks_config['manage_transactions'],
            agent=self.credit_card_transaction_manager(),
            context=[self.parse_bill(), self.manage_card_database(), self.manage_merchants()]
        )

    @crew
    def crew(self) -> Crew:
        #manager = Agent(
        #    config=self.agents_config['crew_master'],
        #    allow_delegation=True
        #)
        return Crew(
            agents = self.agents,
            tasks = self.tasks,
            verbose=True,
            process=Process.sequential,
        )
\end{lstlisting}

And finally, in the main program,

\begin{lstlisting}
import sys
import warnings

from datetime import datetime

from credit_card_bill_injector.crew import CreditCardBillInjector

warnings.filterwarnings("ignore", category=SyntaxWarning, module="pysbd")

from dotenv import load_dotenv
import os

import fitz

def run():
    """
    This function sets up the environment and runs the CrewAI process.
    """
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))
    dotenv_path = os.path.expanduser('~/Projects/smart-home/.env')
    load_dotenv(dotenv_path=dotenv_path, override=True)

    doc = fitz.open('/data/Projects/smart-home/input-document/credit_card_bill.pdf')
    bill_content = ""
    for page in doc:
        bill_content += page.get_text()
    doc.close()

    injector_crew = CreditCardBillInjector()

    result = injector_crew.crew().kickoff(
        inputs={
            'bill_content': bill_content
        }
    )

    print("\n\n########################")
    print("## Here is your Crew's result:")
    print("########################\n")
    print(result)

if __name__ == '__main__':
    run()
\end{lstlisting}

