@article{zhao2023survey,
	title={A survey of large language models},
	author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
	journal={arXiv preprint arXiv:2303.18223},
	year={2023}
}

@misc{lipton2015critical,
	title={A Critical Review of Recurrent Neural Networks for Sequence Learning},
	author={Zachary C. Lipton and John Berkowitz and Charles Elkan},
	year={2015},
	eprint={1506.00019},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{sutskever2014sequence,
	title={Sequence to sequence learning with neural networks},
	author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	journal={Advances in neural information processing systems},
	volume={27},
	year={2014}
}

@misc{touvron2023llama,
	title={LLaMA: Open and Efficient Foundation Language Models},
	author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	year={2023},
	eprint={2302.13971},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{vaswani2017attention,
	title={Attention Is All You Need},
	author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year={2017},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{su2022roformer,
	title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
	author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
	year={2022},
	eprint={2104.09864},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{hu2021lora,
	title={LoRA: Low-Rank Adaptation of Large Language Models},
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{dettmers2023qlora,
	title={QLoRA: Efficient Finetuning of Quantized LLMs},
	author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	journal={arXiv preprint arXiv:2305.14314},
	year={2023}
}

@misc{alpaca,
	author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
	title = {Stanford Alpaca: An Instruction-following LLaMA model},
	year = {2023},
	publisher = {GitHub},
	journal = {GitHub repository},
	howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{vicuna2023,
	title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
	url = {https://lmsys.org/blog/2023-03-30-vicuna/},
	author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
	month = {March},
	year = {2023}
}
  @misc{liu2023improvedllava,
	author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	title={Improved Baselines with Visual Instruction Tuning}, 
	publisher={arXiv:2310.03744},
	year={2023},
}

@inproceedings{liu2023llava,
	author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	title       = {Visual Instruction Tuning},
	booktitle   = {NeurIPS},
	year        = {2023}
}
@article{zhu2023minigpt,
	title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
	author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
	journal={arXiv preprint arXiv:2304.10592},
	year={2023}
}